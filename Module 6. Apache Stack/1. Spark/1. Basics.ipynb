{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "marked-newton",
   "metadata": {},
   "source": [
    "# Spark basics\n",
    "\n",
    "As most of the course is `python` centered we will use `python` frontend (a.k.a. `pyspark`) to interact with the cluster.\n",
    "\n",
    "# Installation\n",
    "\n",
    "## Advised\n",
    "\n",
    "Visit [PySpark download page](https://spark.apache.org/downloads.html) and:\n",
    "- choose latest release\n",
    "- download package locally\n",
    "\n",
    "Install package located under `python` folder (when `conda` environment is active) via (line below means install local package):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c0164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-venture",
   "metadata": {},
   "source": [
    "> Remember to check [compatibility of pyarrow for latest supported Python version](https://arrow.apache.org/docs/python/install.html#python-compatibility)!\n",
    "\n",
    "> Spark has to be installed separately on your system\n",
    "\n",
    "> __Remember to make sure `pip` version plays well with the Spark engine!__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4c9d06",
   "metadata": {},
   "source": [
    "## Unadvised\n",
    "\n",
    "Use `pip` or `conda` manager to install `pyspark`.\n",
    "\n",
    "Please note that:\n",
    "- __You will have to download `Spark` separately__\n",
    "- __Might need to link it appropriately__\n",
    "- __Verify the version installed works with `Spark` we have downloaded__.\n",
    "\n",
    "> All in all, use the advised methods to save yourself some headaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e66f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0b0f35",
   "metadata": {},
   "source": [
    "## findspark\n",
    "\n",
    "As the `spark` version may not be discoverable from `pyspark` package one can use `findspark` utility to connect the components together.\n",
    "\n",
    "- Install `findspark` package\n",
    "- Run `findspark.init()` (which will set up necessary environment variables so `pyspark` can connect to JVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be6d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3caaf48",
   "metadata": {},
   "source": [
    "## Spark config\n",
    "\n",
    "Given all of the steps above, we can set up `spark` distributed engine via:\n",
    "- Frontend (`pyspark` in our case) - usable for application specific tasks and varying configuration\n",
    "- Command line - usable for `spark-submit` and __overriding default values__\n",
    "- Config file - usable as a base config and __when we submit job to the cluster__\n",
    "- Global config file\n",
    "\n",
    "> Above is also a priority list and the config at certain position ovverides values from the ones below it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa5755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "import pyspark\n",
    "\n",
    "cfg = (\n",
    "    pyspark.SparkConf()\n",
    "    # Setting where master node is located [cores for multiprocessing]\n",
    "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
    "    # Setting application name\n",
    "    .setAppName(\"TestApp\")\n",
    "    # Setting config value via string\n",
    "    .set(\"spark.eventLog.enabled\", False)\n",
    "    # Setting environment variables for executors to use\n",
    "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
    "    # Setting memory if this setting was not set previously\n",
    "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
    ")\n",
    "\n",
    "# Getting a single variable\n",
    "print(cfg.get(\"spark.executor.memory\"))\n",
    "# Listing all of them in string readable format\n",
    "print(cfg.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-outreach",
   "metadata": {},
   "source": [
    "# Sessions\n",
    "\n",
    "> PySpark provides session object __which represents UNIFIED connection to Spark cluster__.\n",
    "\n",
    "There are a few ways to set it up:\n",
    "- directly through named/unnamed arguments\n",
    "- __using `SparkConf` object__ (which we created and will use)\n",
    "- Providing `SparkContext` with settings (__deprecated, avoid__)\n",
    "\n",
    "It is used to:\n",
    "- create `DataFrame`s (__main object containing data within cluster__)\n",
    "- __broadcast variables to machines within the cluster__\n",
    "- Run operations across HDFS enabled systems\n",
    "\n",
    "There are some confusing points though...\n",
    "\n",
    "## Contexts, Sessions and all of that...\n",
    "\n",
    "`Spark` and `pyspark` provide a few objects to interact with `Spark` engine:\n",
    "- `pyspark.SparkContext`\n",
    "- __Only `scala`__: `org.apache.spark.sql.SQLContext`\n",
    "- __Only `scala`__: `org.apache.spark.sql.hive.HiveContext`\n",
    "- `pyspark.sql.SparkContext`\n",
    "\n",
    "So what's the deal?\n",
    "\n",
    "### SparkContext\n",
    "\n",
    "> Object used by driver to communicate with cluster manager, executes and coordinates jobs\n",
    "\n",
    "This object is always used and provides __generic `spark` capabilities__\n",
    "\n",
    "### SQLContext\n",
    "\n",
    "> __Given `SparkContext` interact with `SparkSQL` library__\n",
    "\n",
    "One __had to__ provide `SparkContext` to this object in order to interact with SQL-like capabilities (e.g. creating `DataFrame`)\n",
    "\n",
    "### HiveContext\n",
    "\n",
    "> __Extension of SQLContext providing gateway to Hive__\n",
    "\n",
    "Hive is similar in structure to SQL but provides capabilities for data warehouse and is better suited for analyzing large scale data\n",
    "\n",
    "## SparkSession\n",
    "\n",
    "Above differentiation was pretty unsustainable and since `v2.0` of Spark \"one object to rule them all was introduced\" `spark.SparkSession`.\n",
    "\n",
    "In `pyspark` one can use it via `spark.sql.SparkSession` and it has the following capabilities:\n",
    "- __wraps functionalities of ALL CONTEXTS above__\n",
    "- we use a single object to interact with these APIs\n",
    "- __We should use `builder` attribute to obtain appropriate `SparkSession`__\n",
    "\n",
    "After `builder` attribute is run (which constructs appropriate context) one can use it just like `context` objects.\n",
    "\n",
    "Please note:\n",
    "- We use `config` to specify `SparkSession` configuration (essentially Spark configuration)\n",
    "- We use `getOrCreate()` which:\n",
    "    - If no global `Session` exists create a new one with specified config\n",
    "    - If global `Session` exists:\n",
    "        - Get an instance of it\n",
    "        - Apply new configuration to it\n",
    "    - This approach is save as __using multiple context is a bad practice!__ (although possible)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-company",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T14:34:35.447266Z",
     "start_time": "2021-07-10T14:34:35.246044Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c40158",
   "metadata": {},
   "source": [
    "# Data Structures\n",
    "\n",
    "Before diving in we need to talk about `3` available data structures in `spark`:\n",
    "- `RDD` - __R__esilient __D__istributed __D__ataset - fault-tolerant collection of elements that can be operated on in parallel.\n",
    "- `DataFrame` -  dataset organised into named columns. Conceptually equivalent to a table in a relational database or a data frame in R/Python, __but with richer optimisations under the hood.__\n",
    "- `Dataset` - distributed collection of data. Provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQLâ€™s optimized execution engine\n",
    "\n",
    "![](./images/rdd_df_dataset_history.png)\n",
    "\n",
    "# RDD & Core Spark API\n",
    "\n",
    "> __Core and basic of Spark applications with \"low-level\" operations__\n",
    "\n",
    "> __Fault-tolerant collection of elements that can be operated on in parallel.__\n",
    "\n",
    "This structure provides strong typing (via `JVM` objects) and can be constructued in two ways:\n",
    "- __parallelizing existing collection__ (e.g. Python's `list`)\n",
    "- __referencing dataset in external storage__ (anything compatible with Hadoop's InputFormat like HDFS, HBase, Amazon S3, text files etc.)\n",
    "\n",
    "Let's see these options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddDistributedData = session.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "rddDistributedFile = session.sparkContext.textFile(\"lorem.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9009eb8",
   "metadata": {},
   "source": [
    "__Things to note for files__:\n",
    "- __Each file has to be in the same path on each worker node!__ (in our case we are running locally hence this is fine)\n",
    "- All file-based methods operate on:\n",
    "    - directories - `textFile(\"/my/directory\")`\n",
    "    - wildcards - `textFile(\"/my/directory/*.txt\")`\n",
    "    - compressed files - `textFile(\"/my/directory/*.gz\")`\n",
    "- We can change number of partitions created for this file\n",
    "- See API [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.textFile.html#pyspark.SparkContext.textFile)\n",
    "\n",
    "> __Other ways to create RDD from file can be seen in [Spark Context API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#spark-context-apis), e.g. a way to create it from `pickle`__\n",
    "\n",
    "## Lazy Evaulation\n",
    "\n",
    "> Created RDDs __ARE NOT FILES__, they are merely a description of operation that __has to be run at some point__\n",
    "\n",
    "What we did above means:\n",
    "- Parallelize `list` operation\n",
    "- Read from text file `lorem.txt` (__but the read wasn't performed!__)\n",
    "\n",
    "> All of the operations will be run when we __request an ACTION__\n",
    "\n",
    "Actions may include:\n",
    "- return number of lines in file (whole map-reduce went through)\n",
    "- sum the list and return the result\n",
    "\n",
    "## Persist\n",
    "\n",
    "> Persisting is used in order to speed-up computations (saving intermediate results in memory)\n",
    "\n",
    "If we run the line below it means:\n",
    "\n",
    "> Read data file and cache read contents in the memory (if possible)\n",
    "\n",
    "> __If we run \"action\" on the file it will use the cached data (faster) rather than loading data from disk once again!__\n",
    "\n",
    "Rule of thumb: \n",
    "\n",
    "> Use cache when the lineage (operations to run on certain RDD) of your RDD branches out or when an RDD is used multiple times like in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6225bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the operations return self\n",
    "# This allows us to chain operations (we will see it in the next cell)\n",
    "\n",
    "rddDistributedFile = rddDistributedFile.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c57759",
   "metadata": {},
   "source": [
    "> __`.cache()` is the same as `.persist()` with `StorageLevel.MEMORY_ONLY`__\n",
    "\n",
    "There are few other options to store the data:\n",
    "- `MEMORY_ONLY` - keep everything we can in memory otherwise do not cache and compute results\n",
    "- `MEMORY_AND_DISK` - keep everything we can in memory otherwise serialize to disk (__encouraged for long running computations we would like to cache__)\n",
    "- `DISK_ONLY` - cache everything on disk, nothing in memory (__discouraged__)\n",
    "- `MEMORY_ONLY_2` - same as `MEMORY_ONLY` but replicates cache on two cluster nodes for improved fault tolerance (`DISK_ONLY_2` is also available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed77d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.StorageLevel.DISK_ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d15fb2",
   "metadata": {},
   "source": [
    "## MapReduce operations\n",
    "\n",
    "> Given parallelized data structure we can run map-reduce operations on it\n",
    "\n",
    "All of them can be seen [in the documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis), a few interesting ones:\n",
    "- [`rdd.checkpoint()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.checkpoint.html#pyspark.RDD.checkpoint) - will be saved in checkpoint directory and all the operations creating it __are discarded__ (action)\n",
    "- [`rdd.collect()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.collect.html#pyspark.RDD.collect) - __return the structure__ (collect it after operations) (action)\n",
    "- [`rdd.count()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.count.html#pyspark.RDD.count) - count elements in the structure (action)\n",
    "- [`rdd.countByKey()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.countByKey.html#pyspark.RDD.countByKey) - count number of elements for each `key` in `(key, value)` pairs (similar to what the graphic before did)\n",
    "- [`rdd.countByValue()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.countByValue.html#pyspark.RDD.countByValue) - count __how many unique values__ are in this structure (returned as `(value, count)` dictionary)\n",
    "\n",
    "__And the essential ones we will use are:__\n",
    "- [`rdd.map(f)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html#pyspark.RDD.map) - apply function __to each element in the collection__\n",
    "- [`rdd.filter(f)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.filter.html#pyspark.RDD.filter) - __choose values which fulfill `f` function__\n",
    "- [`rdd.flatMap(f)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.flatMap.html#pyspark.RDD.flatMap) - __apply function to each element and `flatten` the list if necessary__\n",
    "- [`rdd.fold(neutralValue, f)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.fold.html#pyspark.RDD.fold) - __given associative function (like `add`) takes every 2 elements together and returns the result__\n",
    "- [`rdd.sortBy(keyfunction)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sortBy.html#pyspark.RDD.sortBy) - sort by specific function which returns some value from the `(key, value)` pair\n",
    "\n",
    "> __PLEASE REFER TO DOCUMENTATION WHEN LOOKING FOR AN OPERATOR! MANY OF THEM ARE ALREADY IMPLEMENTED!__\n",
    "\n",
    "> __TAKE TIME TO COME UP WITH THE OPERATORS NEEDED! EACH OPERATION SAVED MIGHT IMPROVE RUNTIME TREMENDOUSLY!__\n",
    "\n",
    "Let's see an example chaining on data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ffc692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc is standard name for sparkContext\n",
    "# it will be easier to use from now on\n",
    "\n",
    "sc = session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d140a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "data = list(range(10,-11,-1))\n",
    "print(data)\n",
    "\n",
    "result = (\n",
    "    sc.parallelize(data)\n",
    "    .filter(lambda val: val % 3 == 0)\n",
    "    .map(operator.abs)\n",
    "    .fold(0, operator.add)\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229d0632",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([\"b\", \"a\", \"c\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b62bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddDistributedFile.flatMap(lambda text: text.split()).countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d9f88a",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "## Dataset and DataFrame\n",
    "\n",
    "Dataset is a distributed collection of data which provides:\n",
    "- strong typing and powerful lambda functions from `RDD`\n",
    "- __allows for Spark SQL optimized execution engine__\n",
    "\n",
    "It can be created from JVM objects __and manipulated in the same functional manner__.\n",
    "\n",
    "> __`pyspark` has no Dataset API but many benefits of `Dataset` are available for `DataFrame`s DUE TO IT'S DYNAMIC NATURE__\n",
    "\n",
    "DataFrame shortcomings included:\n",
    "- No compile-time safety, hence __you cannot manipulate data of which structure is not specified__\n",
    "\n",
    "> DataFrame is a a  Dataset organized into named columns (__same as for `pd.DataFrame`__)\n",
    "\n",
    "From now on we will use `DataFrame`s (__not `Dataset`, also due to Python's community similarity with `pd.DataFrame`__) to keep our records.\n",
    "\n",
    "See [this discussion](https://stackoverflow.com/questions/31508083/difference-between-dataframe-dataset-and-rdd-in-spark) for an extended description.\n",
    "\n",
    "## Creating DataFrames\n",
    "\n",
    "> __For all of the operations we can use `SparkSession` directly to interact with the cluster!__\n",
    "\n",
    "There are a few options usable for us to read data residing on clusters (__for each node it has to be at the same location if reading from file!__):\n",
    "- [`session.createDataFrame`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html#pyspark.sql.SparkSession.createDataFrame) - create `pyspark.sql.DataFrame` from:\n",
    "    - `RDD`\n",
    "    - `list`\n",
    "    - `pandas.DataFrame`\n",
    "    - __Optionally: with `schema`__ which specifies datatypes and format for data contained within it. See documentation for more info.\n",
    "    - By default `schema` is inferred if possible\n",
    "- [`session.range`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.range.html#pyspark.sql.SparkSession.range) - works like Python's range but distributed and as a `spark.DataFrame`\n",
    "- [`session.sql(query)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.sql.html#pyspark.sql.SparkSession.sql) - __return DataFrame which represents result of `sql` query__\n",
    "- [`session.read.{how_to_read}()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame) - __returns `DataFrameReader` object__ which allows us to read `df` from:\n",
    "    - `json`\n",
    "    - `parquet`\n",
    "    - `csv`\n",
    "    - and many more\n",
    "- [`session.readStream`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.readStream.html#pyspark.sql.SparkSession.readStream) - __used for streaming, we will see it a little later__\n",
    "\n",
    "Let's see some code with `pyspark.sql.DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c41d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = session.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        np.random.randint(0, 100, size=(100, 4)),\n",
    "        columns=list(\"ABCD\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbabd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18c5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show is an action, nothing would be returned without it\n",
    "# Just an operation representing what will happen\n",
    "df.select(\"A\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b0142",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df[\"A\"], df[\"B\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b62de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase column value by one\n",
    "# This operation is shown in the output\n",
    "\n",
    "df.select(df[\"A\"], df[\"B\"] + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e84756",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counted = df.groupby(\"B\").count().persist()\n",
    "counted.filter(counted[\"count\"] > 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952bbf37",
   "metadata": {},
   "source": [
    "## Operations on DataFrame\n",
    "\n",
    "> __`pyspark.sql.DataFrame` supports most of the `pd.DataFrame` operations + the RDD ones__\n",
    "\n",
    "You can see the whole list [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame)\n",
    "\n",
    "> __In general one can work with it similarly to how one works with `pd.DataFrame` objects__\n",
    "\n",
    "there are a few exceptions though...\n",
    "\n",
    "## Running SQL queries\n",
    "\n",
    "> In order to run SQL queries against the DataFrame __we have to register them as `TemporaryViews`__\n",
    "\n",
    "Properties of `TemporaryViews`:\n",
    "- __Session scoped__ - if session runs out of scope so will the views registered for it\n",
    "- One can set up `DataFrame` globally for any `SparkSession` by using `df.createGlobalTempView(\"name_of_database\")`\n",
    "\n",
    "After that, we can run SQL queries against __distributed data across nodes__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3120bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"any_name\")\n",
    "\n",
    "# WE USE SESSION TO RUN QUERIES!\n",
    "sqlDf = session.sql(\"SELECT * FROM any_name\")\n",
    "sqlDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee0c70",
   "metadata": {},
   "source": [
    "# Challenges\n",
    "\n",
    "## Assessment\n",
    "\n",
    "- Check out [`rdd.aggregate`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.aggregate.html#pyspark.RDD.aggregate) method for RDDs.\n",
    "- What is the difference between `forEach` and `map`? Check [this StackOverflow answer](https://stackoverflow.com/questions/354909/is-there-a-difference-between-foreach-and-map) if in doubt\n",
    "- What is the difference between `reduce` and `fold`? check [this StackOverflow answer](https://stackoverflow.com/a/36060141/10886420). Which one is \"safer\" to use?\n",
    "- Which operations on RDDs induce `shuffle` and why is it a problem? See [here](https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations) for more info\n",
    "- Check how to use [Hive](https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html) with PySpark. What Hive is and how it differs from SQL?\n",
    "- Check out how to specify schema programmaticaly (presented [in this tutorial](https://spark.apache.org/docs/latest/sql-getting-started.html#programmatically-specifying-the-schema)). What are the upsides/downsides of using it?\n",
    "\n",
    "## Non-assessment\n",
    "\n",
    "- Read more about multiple `SparkContext`s and `SparkSession`s and why would we need it in some... contexts. Check it [over here](https://www.waitingforcode.com/apache-spark-sql/multiple-sparksession-one-sparkcontext/read)\n",
    "- What is [`rdd.meanApprox`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.meanApprox.html#pyspark.RDD.meanApprox) and why might we need it?\n",
    "- Generally discouraged, but what are the options to share data between tasks and nodes in the cluster? Check out [this part of RDD tutorial](https://spark.apache.org/docs/latest/rdd-programming-guide.html#shared-variables)\n",
    "- Check [performance tuning options for `spark.sql`](https://spark.apache.org/docs/latest/sql-performance-tuning.html). One can use them when creating `pyspark.SparkConf()` object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-AiCore] *",
   "language": "python",
   "name": "conda-env-.conda-AiCore-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
