{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prometheus\n",
    "\n",
    "> __Prometheus is an open source monitoring and alerting toolkit gathering and processing data locally__\n",
    "\n",
    "A few facts about it:\n",
    "- Written in `golang`\n",
    "- Provides APIs for different languages (including Python), __but__...\n",
    "- We use our browser to query data using Prometheus specific language called PromQL\n",
    "- Above __dashboard__ resides on `localhost:9090` by default\n",
    "\n",
    "![](images/prometheus_architecture.png)\n",
    "\n",
    "## What's going on above?\n",
    "\n",
    "- Prometheus scrapes data (like metrics):\n",
    "    - from short lived jobs via `push gateway`\n",
    "    - from long running jobs directly\n",
    "- __All samples (values with timestamps) are stored locally__ (together with necessary metadata)\n",
    "- Runs predefined rules on collected data:\n",
    "    - Gather and aggregate new records\n",
    "    - Process them and send alerts\n",
    "- Prometheus's API consumers are used to visualize data\n",
    "\n",
    "## When to use it?\n",
    "\n",
    "- __Recording numerical time series__, this could be:\n",
    "    - various training metrics gathered across epochs/batches\n",
    "    - hardware related data\n",
    "    - network traffic and other statistics\n",
    "- __To debug infrastructure during network outages etc.__ (if interested, you can read about Slack's outage [here](https://slack.engineering/slacks-outage-on-january-4th-2021/))\n",
    "\n",
    "> __Data is stored locally (on each node) AND DOES NOT RELY ON NETWORK STORAGE__\n",
    "\n",
    "Due to above, if something fails, you always have access to the data __on each node__ (as each Prometheus server is self-contained and independent).\n",
    "\n",
    "__When shouldn't we use it?__\n",
    "\n",
    "- __You need very detailed data coming really fast__ (per request metrics with a lot of requests from users, exact billing when every milisecond counts etc.)\n",
    "\n",
    "That's because:\n",
    "- Prometheus is desgined to scrape data every few seconds\n",
    "- Data is kept in local storage which can fill up really fast (in this case think about large remote storage and data rotation)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "\n",
    "> __All prometheus data is stored as timestamped timeseries differentiated by metric and (optionally) label__\n",
    "\n",
    "- Metric names and labels should be alphanumerical\n",
    "- __Samples are `float64` (`double`) numerical types__\n",
    "- __Timestamps are in milliseconds__\n",
    "\n",
    "```bash\n",
    "api_http_requests_total{method=\"POST\", handler=\"/messages\"}\n",
    "```\n",
    "\n",
    "In the above case:\n",
    "- `api_http_requests_total` - metric name\n",
    "- `method=\"POST\"` - label `method` which is equal to `\"POST\"`\n",
    "- `handler=\"/messages\"` - label `handler` which is equal to `\"/messages\"`\n",
    "\n",
    "## Metrics\n",
    "\n",
    "> Prometheus provides `4` metrics out of the box\n",
    "\n",
    "__Note:__ Prometheus server does not differentiate between metrics (it only keeps the data), metrics are used by the client libraries (once again `4` client libraries provided for `golang`, `java`, __`python`__, `ruby`)\n",
    "\n",
    "### Counter\n",
    "\n",
    "> __Monotonically increasing counter (can be restarted)__\n",
    "\n",
    "Useful for:\n",
    "- number of requests\n",
    "- tasks completed\n",
    "- __anything which can only grow OR start a new__\n",
    "\n",
    "### Gauge\n",
    "\n",
    "> __Single value which can increase & decrease__\n",
    "\n",
    "Useful for:\n",
    "- memory usage monitoring\n",
    "- temperature\n",
    "- __anything which can change value arbitrarily__\n",
    "\n",
    "### Histogram\n",
    "\n",
    "> __Samples observations and groups them in buckets (which you can configure)__\n",
    "\n",
    "Let's say our historgram metric is named `our_super_histogram`. In this case the following operations on histogram are available (__notice we add suffixes to metric name!__):\n",
    "- `our_super_histogram_bucket{le=\"<upper inclusive bound>\"}` - cumulative counters for the observation buckets\n",
    "- `our_super_histogram_sum` - sum of all values\n",
    "- `out_super_histogram_count` - count of observations\n",
    "\n",
    "### Summary\n",
    "\n",
    "> __Samples observations and provides them as a sliding time window__\n",
    "\n",
    "- Provides `sum` and `count` like histogram\n",
    "- `out_super_summary_quantiles{quantile=\"value\"}` - quantile of observations (one can do something similar for histogram but using functions)\n",
    "\n",
    "### Functions\n",
    "\n",
    "There are also functions one can run to query for things like `day_of_month()`, we will talk about them in more detail later\n",
    "\n",
    "## Jobs and instances\n",
    "\n",
    "> __An instance is an endpoint you can scrape data from__, single process\n",
    "> __A job is a collections of the same instances__, those are usually replicated for reliability/flexbility\n",
    "\n",
    "https://prometheus.io/docs/concepts/jobs_instances/ here more if needed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# First contact\n",
    "\n",
    "## Installation\n",
    "\n",
    "> Prometheus was written in `golang`, hence __it is contained in a single compiled executable__\n",
    "\n",
    "Due to above, it's installation & deployment is really simple and can be performed efficiently in many different scenarios:\n",
    "\n",
    "- Go to [their download page](https://prometheus.io/download/) and download appropriate binary\n",
    "    - if you're on Mac, it's the download labelled Darwin\n",
    "- Check your OS's packages, some of those are officially maintained (e.g. `prometheus` for arch linux)\n",
    "- __Run prometheus inside Docker container__\n",
    "\n",
    "We will use the last option. Run command below in your command line:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "docker run --rm -p 9090:9090 prom/prometheus"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "What it does:\n",
    "- bind container's `9090` port to `9090` port on the localhost\n",
    "- remove container (`--rm` flag) when you kill the process\n",
    "\n",
    "__Simply go to [`localhost:9090`](http://localhost:9090) in your browser__ and you should see the following (you can mark every checkbox the same way we do):\n",
    "\n",
    "![](images/prometheus_webui.png)\n",
    "\n",
    "Start typing something into the `Expression`, you should see autocompletion with possibilities to query:\n",
    "\n",
    "![](images/prometheus_webui_autocompletion.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise\n",
    "\n",
    "- Find `20` functions starting with `20` different letters and run them\n",
    "- Check their graph representation\n",
    "\n",
    "Understand what those do, consult documentation if needed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prometheus configuration\n",
    "\n",
    "## Configuration file\n",
    "\n",
    "Until now, we didn't know any details of what just happened, but that's about to change.\n",
    "\n",
    "> __Prometheus server is configured via [YAML](https://en.wikipedia.org/wiki/YAML) files__\n",
    "\n",
    "Previously, we ran Prometheus __with default configuration file__ (one can see it in [`localhost:9090/config`](http://localhost:9090/config).\n",
    "\n",
    "For brevity, let's take a look at this config file to have a feel of what's possible:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Section with default values\r\n",
    "global:\r\n",
    "  scrape_interval: 15s # How frequently to scrape targets from jobs\r\n",
    "  scrape_timeout: 10s # If there is no response from instance do not try to scrape\r\n",
    "  evaluation_interval: 15s # How frequently to evaluate rules (e.g. reload graphs with new data)\r\n",
    "# Prometheus alert manager, left for now\r\n",
    "alerting:\r\n",
    "  alertmanagers:\r\n",
    "  - follow_redirects: true\r\n",
    "    scheme: http\r\n",
    "    timeout: 10s\r\n",
    "    api_version: v2\r\n",
    "    static_configs:\r\n",
    "    - targets: []\r\n",
    "# Specific configuration for jobs\r\n",
    "scrape_configs:\r\n",
    "- job_name: prometheus # Name of the job, can be anything\r\n",
    "  honor_timestamps: true # Use timestamps provided by job\r\n",
    "  scrape_interval: 15s # As before, but for this job\r\n",
    "  scrape_timeout: 10s # ^\r\n",
    "  metrics_path: /metrics # Where metrics are located w.r.t. port (localhost:9090/metrics)\r\n",
    "  scheme: http # Configures the protocol scheme used for requests (localhost is http)\r\n",
    "  follow_redirects: true\r\n",
    "  static_configs:\r\n",
    "  - targets:\r\n",
    "    - localhost:9090"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> __Prometheus provides A LOT of configuration options, check all of them [here](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config)__\n",
    "\n",
    "A few things to note about config file to keep in mind:\n",
    "- __Prometheus reloads it's configuration based on the config file automatically__ (you can change it live or even reload it by accessing `/-/reload`)\n",
    "- __If the file is not correctly formed IT WILL NOT BE UPDATED__ \n",
    "- `global` is used for everything if not specified (especially inside `scrape_configs`)\n",
    "\n",
    "### [scrape_configs](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config)\n",
    "\n",
    "> Specifies __sets of targets__ (what should be scraped) and parameters describing how to do it for each target\n",
    "\n",
    "Targets can be defined __statically__ or __dynamically__\n",
    "\n",
    "- `statically` - specified via `port` etc.\n",
    "- `dynamically` (service discovery) - using __service discovery mechanisms__ (for example all jobs labeled by `deep-learning`)\n",
    "\n",
    "A lot of integrations for scraping are provided, some of those are:\n",
    "- [`consul_sd_config`](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#consul_sd_config) - retrieve scraping targets from HashiCorp's [Consul](https://www.consul.io/) used for service discovery and network setup\n",
    "- [`dockerswarm_sd_config`](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dockerswarm_sd_config) - used with [Docker's Swarm mode](https://docs.docker.com/engine/swarm/) which allows us to connect and orchestrate many containers as one application\n",
    "- [`ec2_sd_config`](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config)- retrieve scrape targets from EC2 instances\n",
    "- [`kubernetes_sd_config`](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config) - configure scrape targets from Kubernetes REST API\n",
    "\n",
    "\n",
    "https://prometheus.io/docs/prometheus/latest/configuration/configuration/#static_config anything below that should be added?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Command line\n",
    "\n",
    "> Instance of Prometheus server itself can be configured via command line flags\n",
    "\n",
    "You can run the command below to see available options:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "docker container run --rm prom/prometheus --help"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "usage: prometheus [<flags>]\n",
      "\n",
      "The Prometheus monitoring server\n",
      "\n",
      "Flags:\n",
      "  -h, --help                     Show context-sensitive help (also try\n",
      "                                 --help-long and --help-man).\n",
      "      --version                  Show application version.\n",
      "      --config.file=\"prometheus.yml\"  \n",
      "                                 Prometheus configuration file path.\n",
      "      --web.listen-address=\"0.0.0.0:9090\"  \n",
      "                                 Address to listen on for UI, API, and\n",
      "                                 telemetry.\n",
      "      --web.config.file=\"\"       [EXPERIMENTAL] Path to configuration file that\n",
      "                                 can enable TLS or authentication.\n",
      "      --web.read-timeout=5m      Maximum duration before timing out read of the\n",
      "                                 request, and closing idle connections.\n",
      "      --web.max-connections=512  Maximum number of simultaneous connections.\n",
      "      --web.external-url=<URL>   The URL under which Prometheus is externally\n",
      "                                 reachable (for example, if Prometheus is served\n",
      "                                 via a reverse proxy). Used for generating\n",
      "                                 relative and absolute links back to Prometheus\n",
      "                                 itself. If the URL has a path portion, it will\n",
      "                                 be used to prefix all HTTP endpoints served by\n",
      "                                 Prometheus. If omitted, relevant URL components\n",
      "                                 will be derived automatically.\n",
      "      --web.route-prefix=<path>  Prefix for the internal routes of web\n",
      "                                 endpoints. Defaults to path of\n",
      "                                 --web.external-url.\n",
      "      --web.user-assets=<path>   Path to static asset directory, available at\n",
      "                                 /user.\n",
      "      --web.enable-lifecycle     Enable shutdown and reload via HTTP request.\n",
      "      --web.enable-admin-api     Enable API endpoints for admin control actions.\n",
      "      --web.console.templates=\"consoles\"  \n",
      "                                 Path to the console template directory,\n",
      "                                 available at /consoles.\n",
      "      --web.console.libraries=\"console_libraries\"  \n",
      "                                 Path to the console library directory.\n",
      "      --web.page-title=\"Prometheus Time Series Collection and Processing Server\"  \n",
      "                                 Document title of Prometheus instance.\n",
      "      --web.cors.origin=\".*\"     Regex for CORS origin. It is fully anchored.\n",
      "                                 Example: 'https?://(domain1|domain2)\\.com'\n",
      "      --storage.tsdb.path=\"data/\"  \n",
      "                                 Base path for metrics storage.\n",
      "      --storage.tsdb.retention=STORAGE.TSDB.RETENTION  \n",
      "                                 [DEPRECATED] How long to retain samples in\n",
      "                                 storage. This flag has been deprecated, use\n",
      "                                 \"storage.tsdb.retention.time\" instead.\n",
      "      --storage.tsdb.retention.time=STORAGE.TSDB.RETENTION.TIME  \n",
      "                                 How long to retain samples in storage. When\n",
      "                                 this flag is set it overrides\n",
      "                                 \"storage.tsdb.retention\". If neither this flag\n",
      "                                 nor \"storage.tsdb.retention\" nor\n",
      "                                 \"storage.tsdb.retention.size\" is set, the\n",
      "                                 retention time defaults to 15d. Units\n",
      "                                 Supported: y, w, d, h, m, s, ms.\n",
      "      --storage.tsdb.retention.size=STORAGE.TSDB.RETENTION.SIZE  \n",
      "                                 [EXPERIMENTAL] Maximum number of bytes that can\n",
      "                                 be stored for blocks. A unit is required,\n",
      "                                 supported units: B, KB, MB, GB, TB, PB, EB. Ex:\n",
      "                                 \"512MB\". This flag is experimental and can be\n",
      "                                 changed in future releases.\n",
      "      --storage.tsdb.no-lockfile  \n",
      "                                 Do not create lockfile in data directory.\n",
      "      --storage.tsdb.allow-overlapping-blocks  \n",
      "                                 [EXPERIMENTAL] Allow overlapping blocks, which\n",
      "                                 in turn enables vertical compaction and\n",
      "                                 vertical query merge.\n",
      "      --storage.tsdb.wal-compression  \n",
      "                                 Compress the tsdb WAL.\n",
      "      --storage.remote.flush-deadline=<duration>  \n",
      "                                 How long to wait flushing sample on shutdown or\n",
      "                                 config reload.\n",
      "      --storage.remote.read-sample-limit=5e7  \n",
      "                                 Maximum overall number of samples to return via\n",
      "                                 the remote read interface, in a single query. 0\n",
      "                                 means no limit. This limit is ignored for\n",
      "                                 streamed response types.\n",
      "      --storage.remote.read-concurrent-limit=10  \n",
      "                                 Maximum number of concurrent remote read calls.\n",
      "                                 0 means no limit.\n",
      "      --storage.remote.read-max-bytes-in-frame=1048576  \n",
      "                                 Maximum number of bytes in a single frame for\n",
      "                                 streaming remote read response types before\n",
      "                                 marshalling. Note that client might have limit\n",
      "                                 on frame size as well. 1MB as recommended by\n",
      "                                 protobuf by default.\n",
      "      --storage.exemplars.exemplars-limit=100000  \n",
      "                                 [EXPERIMENTAL] Maximum number of exemplars to\n",
      "                                 store in in-memory exemplar storage total. 0\n",
      "                                 disables the exemplar storage. This flag is\n",
      "                                 effective only with\n",
      "                                 --enable-feature=exemplar-storage.\n",
      "      --rules.alert.for-outage-tolerance=1h  \n",
      "                                 Max time to tolerate prometheus outage for\n",
      "                                 restoring \"for\" state of alert.\n",
      "      --rules.alert.for-grace-period=10m  \n",
      "                                 Minimum duration between alert and restored\n",
      "                                 \"for\" state. This is maintained only for alerts\n",
      "                                 with configured \"for\" time greater than grace\n",
      "                                 period.\n",
      "      --rules.alert.resend-delay=1m  \n",
      "                                 Minimum amount of time to wait before resending\n",
      "                                 an alert to Alertmanager.\n",
      "      --alertmanager.notification-queue-capacity=10000  \n",
      "                                 The capacity of the queue for pending\n",
      "                                 Alertmanager notifications.\n",
      "      --query.lookback-delta=5m  The maximum lookback duration for retrieving\n",
      "                                 metrics during expression evaluations and\n",
      "                                 federation.\n",
      "      --query.timeout=2m         Maximum time a query may take before being\n",
      "                                 aborted.\n",
      "      --query.max-concurrency=20  \n",
      "                                 Maximum number of queries executed\n",
      "                                 concurrently.\n",
      "      --query.max-samples=50000000  \n",
      "                                 Maximum number of samples a single query can\n",
      "                                 load into memory. Note that queries will fail\n",
      "                                 if they try to load more samples than this into\n",
      "                                 memory, so this also limits the number of\n",
      "                                 samples a query can return.\n",
      "      --enable-feature= ...      Comma separated feature names to enable. Valid\n",
      "                                 options: promql-at-modifier,\n",
      "                                 promql-negative-offset, remote-write-receiver,\n",
      "                                 exemplar-storage. See\n",
      "                                 https://prometheus.io/docs/prometheus/latest/disabled_features/\n",
      "                                 for more details.\n",
      "      --log.level=info           Only log messages with the given severity or\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                 above. One of: [debug, info, warn, error]\n",
      "      --log.format=logfmt        Output format of log messages. One of: [logfmt,\n",
      "                                 json]\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T18:54:21.063950Z",
     "start_time": "2021-04-11T18:54:20.306793Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Most notable are:\n",
    "\n",
    "- `--web.read-timeout=5m` - Maximum duration before timing out read of the request, and closing idle connections\n",
    "- `--web.max-connections=512` - Maximum number of simultaneous connections\n",
    "- `--web.enable-lifecycle` - Enable shutdown and reload via HTTP request (requests to `/-/reload` mentioned previously)\n",
    "- `--web.page-title=\"...\"` - Change header of the webpage we ran previously\n",
    "- `--storage.tsdb.retention.time` - How long should we keep the data (default: `15` days)\n",
    "- `--storage.remote.read-concurrent-limit=10` - How many targets can be read simultaneuosly\n",
    "- `--log.level=info` - Verbosity of Prometheus server, one of `[debug, info, warn, error]` can be set\n",
    "\n",
    "__Note: Those flags have their categories first, followed by more categories (optionally) and option as the last one__\n",
    "\n",
    "Above is due to Prometheus's structure and `golang` as a language of choice."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exporters\n",
    "\n",
    "> Exporters export existing metrics from third-party systems and make them available as Prometheus metrics\n",
    "\n",
    "There are hundreds of exporters currently:\n",
    "- official - best practices and verifired by Prometheus; __always pick them if possible__\n",
    "- unofficial - working, not verified for best practices or may have overlapping functionalities\n",
    "- in development - to be released as either of the two above\n",
    "\n",
    "You can see [short list here](https://prometheus.io/docs/instrumenting/exporters/) and much longer one [here](https://github.com/prometheus/prometheus/wiki/Default-port-allocations). Important things to keep in mind:\n",
    "- __Most of the exporters occupy ports `9100`-`9999`__ and any new exporter should use it if any is available (see longer list above)\n",
    "- There are a few exporters outside the standard range (once again, see longer list above)\n",
    "\n",
    "To learn more, we will now use one of the common exporter __[Node exporter](https://github.com/prometheus/node_exporter)__:\n",
    "\n",
    "> Node Exporter is a Prometheus supported exporter for hardware and OS metrics exposed by *NIX kernels"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exporting *NIX metrics\n",
    "\n",
    "> `Node` exporter is a single static binary one can download and run straight from the workstation\n",
    "\n",
    "- Following command will download the exporter, unpack the `.tar.gz` archive (__we assume you have *NIX system!__).\n",
    "- You may run those commands anywhere you want, cell below uses temporary directory.\n",
    "- If you have Windows, you should use [this exporter](https://github.com/prometheus-community/windows_exporter).\n",
    "- If on MacOS, run `brew install node_exporter`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "mkdir tmp\r\n",
    "cd $tmpdir\r\n",
    "wget https://github.com/prometheus/node_exporter/releases/download/v1.1.2/node_exporter-1.1.2.linux-amd64.tar.gz\r\n",
    "tar xvfz node_exporter-1.1.2.linux-amd64.tar.gz\r\n",
    "./node_exporter-1.1.2.linux-amd64/node_exporter --help\r\n",
    "cd .."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2021-04-12 14:50:19--  https://github.com/prometheus/node_exporter/releases/download/v1.1.2/node_exporter-1.1.2.linux-amd64.tar.gz\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-releases.githubusercontent.com/9524057/715b1a00-7d9f-11eb-8cfa-533c911cfe9a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210412%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210412T125019Z&X-Amz-Expires=300&X-Amz-Signature=afb080ed4433852dd36268cfbf30bf1a961ae75d295d08de7b1ce0128fac6bae&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=9524057&response-content-disposition=attachment%3B%20filename%3Dnode_exporter-1.1.2.linux-amd64.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2021-04-12 14:50:19--  https://github-releases.githubusercontent.com/9524057/715b1a00-7d9f-11eb-8cfa-533c911cfe9a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210412%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210412T125019Z&X-Amz-Expires=300&X-Amz-Signature=afb080ed4433852dd36268cfbf30bf1a961ae75d295d08de7b1ce0128fac6bae&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=9524057&response-content-disposition=attachment%3B%20filename%3Dnode_exporter-1.1.2.linux-amd64.tar.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.111.154, 185.199.108.154, 185.199.109.154, ...\n",
      "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.111.154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9246179 (8.8M) [application/octet-stream]\n",
      "Saving to: ‘node_exporter-1.1.2.linux-amd64.tar.gz.1’\n",
      "\n",
      "node_exporter-1.1.2 100%[===================>]   8.82M  2.15MB/s    in 4.6s    \n",
      "\n",
      "2021-04-12 14:50:25 (1.91 MB/s) - ‘node_exporter-1.1.2.linux-amd64.tar.gz.1’ saved [9246179/9246179]\n",
      "\n",
      "node_exporter-1.1.2.linux-amd64/\n",
      "node_exporter-1.1.2.linux-amd64/LICENSE\n",
      "node_exporter-1.1.2.linux-amd64/NOTICE\n",
      "node_exporter-1.1.2.linux-amd64/node_exporter\n",
      "usage: node_exporter [<flags>]\n",
      "\n",
      "Flags:\n",
      "  -h, --help                     Show context-sensitive help (also try\n",
      "                                 --help-long and --help-man).\n",
      "      --collector.bcache.priorityStats  \n",
      "                                 Expose expensive priority stats.\n",
      "      --collector.cpu.info       Enables metric cpu_info\n",
      "      --collector.cpu.info.flags-include=COLLECTOR.CPU.INFO.FLAGS-INCLUDE  \n",
      "                                 Filter the `flags` field in cpuInfo with a\n",
      "                                 value that must be a regular expression\n",
      "      --collector.cpu.info.bugs-include=COLLECTOR.CPU.INFO.BUGS-INCLUDE  \n",
      "                                 Filter the `bugs` field in cpuInfo with a value\n",
      "                                 that must be a regular expression\n",
      "      --collector.diskstats.ignored-devices=\"^(ram|loop|fd|(h|s|v|xv)d[a-z]|nvme\\\\d+n\\\\d+p)\\\\d+$\"  \n",
      "                                 Regexp of devices to ignore for diskstats.\n",
      "      --collector.filesystem.ignored-mount-points=\"^/(dev|proc|sys|var/lib/docker/.+)($|/)\"  \n",
      "                                 Regexp of mount points to ignore for filesystem\n",
      "                                 collector.\n",
      "      --collector.filesystem.ignored-fs-types=\"^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$\"  \n",
      "                                 Regexp of filesystem types to ignore for\n",
      "                                 filesystem collector.\n",
      "      --collector.ipvs.backend-labels=\"local_address,local_port,remote_address,remote_port,proto,local_mark\"  \n",
      "                                 Comma separated list for IPVS backend stats\n",
      "                                 labels.\n",
      "      --collector.netclass.ignored-devices=\"^$\"  \n",
      "                                 Regexp of net devices to ignore for netclass\n",
      "                                 collector.\n",
      "      --collector.netdev.device-include=COLLECTOR.NETDEV.DEVICE-INCLUDE  \n",
      "                                 Regexp of net devices to include (mutually\n",
      "                                 exclusive to device-exclude).\n",
      "      --collector.netdev.device-exclude=COLLECTOR.NETDEV.DEVICE-EXCLUDE  \n",
      "                                 Regexp of net devices to exclude (mutually\n",
      "                                 exclusive to device-include).\n",
      "      --collector.netstat.fields=\"^(.*_(InErrors|InErrs)|Ip_Forwarding|Ip(6|Ext)_(InOctets|OutOctets)|Icmp6?_(InMsgs|OutMsgs)|TcpExt_(Listen.*|Syncookies.*|TCPSynRetrans)|Tcp_(ActiveOpens|InSegs|OutSegs|OutRsts|PassiveOpens|RetransSegs|CurrEstab)|Udp6?_(InDatagrams|OutDatagrams|NoPorts|RcvbufErrors|SndbufErrors))$\"  \n",
      "                                 Regexp of fields to return for netstat\n",
      "                                 collector.\n",
      "      --collector.ntp.server=\"127.0.0.1\"  \n",
      "                                 NTP server to use for ntp collector\n",
      "      --collector.ntp.protocol-version=4  \n",
      "                                 NTP protocol version\n",
      "      --collector.ntp.server-is-local  \n",
      "                                 Certify that collector.ntp.server address is\n",
      "                                 not a public ntp server\n",
      "      --collector.ntp.ip-ttl=1   IP TTL to use while sending NTP query\n",
      "      --collector.ntp.max-distance=3.46608s  \n",
      "                                 Max accumulated distance to the root\n",
      "      --collector.ntp.local-offset-tolerance=1ms  \n",
      "                                 Offset between local clock and local ntpd time\n",
      "                                 to tolerate\n",
      "      --path.procfs=\"/proc\"      procfs mountpoint.\n",
      "      --path.sysfs=\"/sys\"        sysfs mountpoint.\n",
      "      --path.rootfs=\"/\"          rootfs mountpoint.\n",
      "      --collector.perf.cpus=\"\"   List of CPUs from which perf metrics should be\n",
      "                                 collected\n",
      "      --collector.perf.tracepoint=COLLECTOR.PERF.TRACEPOINT ...  \n",
      "                                 perf tracepoint that should be collected\n",
      "      --collector.powersupply.ignored-supplies=\"^$\"  \n",
      "                                 Regexp of power supplies to ignore for\n",
      "                                 powersupplyclass collector.\n",
      "      --collector.qdisc.fixtures=\"\"  \n",
      "                                 test fixtures to use for qdisc collector\n",
      "                                 end-to-end testing\n",
      "      --collector.runit.servicedir=\"/etc/service\"  \n",
      "                                 Path to runit service directory.\n",
      "      --collector.supervisord.url=\"http://localhost:9001/RPC2\"  \n",
      "                                 XML RPC endpoint.\n",
      "      --collector.systemd.unit-include=\".+\"  \n",
      "                                 Regexp of systemd units to include. Units must\n",
      "                                 both match include and not match exclude to be\n",
      "                                 included.\n",
      "      --collector.systemd.unit-exclude=\".+\\\\.(automount|device|mount|scope|slice)\"  \n",
      "                                 Regexp of systemd units to exclude. Units must\n",
      "                                 both match include and not match exclude to be\n",
      "                                 included.\n",
      "      --collector.systemd.enable-task-metrics  \n",
      "                                 Enables service unit tasks metrics\n",
      "                                 unit_tasks_current and unit_tasks_max\n",
      "      --collector.systemd.enable-restarts-metrics  \n",
      "                                 Enables service unit metric\n",
      "                                 service_restart_total\n",
      "      --collector.systemd.enable-start-time-metrics  \n",
      "                                 Enables service unit metric\n",
      "                                 unit_start_time_seconds\n",
      "      --collector.textfile.directory=\"\"  \n",
      "                                 Directory to read text files with metrics from.\n",
      "      --collector.vmstat.fields=\"^(oom_kill|pgpg|pswp|pg.*fault).*\"  \n",
      "                                 Regexp of fields to return for vmstat\n",
      "                                 collector.\n",
      "      --collector.wifi.fixtures=\"\"  \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                 test fixtures to use for wifi collector metrics\n",
      "      --collector.arp            Enable the arp collector (default: enabled).\n",
      "      --collector.bcache         Enable the bcache collector (default: enabled).\n",
      "      --collector.bonding        Enable the bonding collector (default:\n",
      "                                 enabled).\n",
      "      --collector.btrfs          Enable the btrfs collector (default: enabled).\n",
      "      --collector.buddyinfo      Enable the buddyinfo collector (default:\n",
      "                                 disabled).\n",
      "      --collector.conntrack      Enable the conntrack collector (default:\n",
      "                                 enabled).\n",
      "      --collector.cpu            Enable the cpu collector (default: enabled).\n",
      "      --collector.cpufreq        Enable the cpufreq collector (default:\n",
      "                                 enabled).\n",
      "      --collector.diskstats      Enable the diskstats collector (default:\n",
      "                                 enabled).\n",
      "      --collector.drbd           Enable the drbd collector (default: disabled).\n",
      "      --collector.edac           Enable the edac collector (default: enabled).\n",
      "      --collector.entropy        Enable the entropy collector (default:\n",
      "                                 enabled).\n",
      "      --collector.fibrechannel   Enable the fibrechannel collector (default:\n",
      "                                 enabled).\n",
      "      --collector.filefd         Enable the filefd collector (default: enabled).\n",
      "      --collector.filesystem     Enable the filesystem collector (default:\n",
      "                                 enabled).\n",
      "      --collector.hwmon          Enable the hwmon collector (default: enabled).\n",
      "      --collector.infiniband     Enable the infiniband collector (default:\n",
      "                                 enabled).\n",
      "      --collector.interrupts     Enable the interrupts collector (default:\n",
      "                                 disabled).\n",
      "      --collector.ipvs           Enable the ipvs collector (default: enabled).\n",
      "      --collector.ksmd           Enable the ksmd collector (default: disabled).\n",
      "      --collector.loadavg        Enable the loadavg collector (default:\n",
      "                                 enabled).\n",
      "      --collector.logind         Enable the logind collector (default:\n",
      "                                 disabled).\n",
      "      --collector.mdadm          Enable the mdadm collector (default: enabled).\n",
      "      --collector.meminfo        Enable the meminfo collector (default:\n",
      "                                 enabled).\n",
      "      --collector.meminfo_numa   Enable the meminfo_numa collector (default:\n",
      "                                 disabled).\n",
      "      --collector.mountstats     Enable the mountstats collector (default:\n",
      "                                 disabled).\n",
      "      --collector.netclass       Enable the netclass collector (default:\n",
      "                                 enabled).\n",
      "      --collector.netdev         Enable the netdev collector (default: enabled).\n",
      "      --collector.netstat        Enable the netstat collector (default:\n",
      "                                 enabled).\n",
      "      --collector.network_route  Enable the network_route collector (default:\n",
      "                                 disabled).\n",
      "      --collector.nfs            Enable the nfs collector (default: enabled).\n",
      "      --collector.nfsd           Enable the nfsd collector (default: enabled).\n",
      "      --collector.ntp            Enable the ntp collector (default: disabled).\n",
      "      --collector.perf           Enable the perf collector (default: disabled).\n",
      "      --collector.powersupplyclass  \n",
      "                                 Enable the powersupplyclass collector (default:\n",
      "                                 enabled).\n",
      "      --collector.pressure       Enable the pressure collector (default:\n",
      "                                 enabled).\n",
      "      --collector.processes      Enable the processes collector (default:\n",
      "                                 disabled).\n",
      "      --collector.qdisc          Enable the qdisc collector (default: disabled).\n",
      "      --collector.rapl           Enable the rapl collector (default: enabled).\n",
      "      --collector.runit          Enable the runit collector (default: disabled).\n",
      "      --collector.schedstat      Enable the schedstat collector (default:\n",
      "                                 enabled).\n",
      "      --collector.sockstat       Enable the sockstat collector (default:\n",
      "                                 enabled).\n",
      "      --collector.softnet        Enable the softnet collector (default:\n",
      "                                 enabled).\n",
      "      --collector.stat           Enable the stat collector (default: enabled).\n",
      "      --collector.supervisord    Enable the supervisord collector (default:\n",
      "                                 disabled).\n",
      "      --collector.systemd        Enable the systemd collector (default:\n",
      "                                 disabled).\n",
      "      --collector.tcpstat        Enable the tcpstat collector (default:\n",
      "                                 disabled).\n",
      "      --collector.textfile       Enable the textfile collector (default:\n",
      "                                 enabled).\n",
      "      --collector.thermal_zone   Enable the thermal_zone collector (default:\n",
      "                                 enabled).\n",
      "      --collector.time           Enable the time collector (default: enabled).\n",
      "      --collector.timex          Enable the timex collector (default: enabled).\n",
      "      --collector.udp_queues     Enable the udp_queues collector (default:\n",
      "                                 enabled).\n",
      "      --collector.uname          Enable the uname collector (default: enabled).\n",
      "      --collector.vmstat         Enable the vmstat collector (default: enabled).\n",
      "      --collector.wifi           Enable the wifi collector (default: disabled).\n",
      "      --collector.xfs            Enable the xfs collector (default: enabled).\n",
      "      --collector.zfs            Enable the zfs collector (default: enabled).\n",
      "      --collector.zoneinfo       Enable the zoneinfo collector (default:\n",
      "                                 disabled).\n",
      "      --web.listen-address=\":9100\"  \n",
      "                                 Address on which to expose metrics and web\n",
      "                                 interface.\n",
      "      --web.telemetry-path=\"/metrics\"  \n",
      "                                 Path under which to expose metrics.\n",
      "      --web.disable-exporter-metrics  \n",
      "                                 Exclude metrics about the exporter itself\n",
      "                                 (promhttp_*, process_*, go_*).\n",
      "      --web.max-requests=40      Maximum number of parallel scrape requests. Use\n",
      "                                 0 to disable.\n",
      "      --collector.disable-defaults  \n",
      "                                 Set all collectors to disabled by default.\n",
      "      --web.config=\"\"            [EXPERIMENTAL] Path to config yaml file that\n",
      "                                 can enable TLS or authentication.\n",
      "      --log.level=info           Only log messages with the given severity or\n",
      "                                 above. One of: [debug, info, warn, error]\n",
      "      --log.format=logfmt        Output format of log messages. One of: [logfmt,\n",
      "                                 json]\n",
      "      --version                  Show application version.\n",
      "\n",
      "/tmp\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T12:50:25.449113Z",
     "start_time": "2021-04-12T12:50:19.416349Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Note:__ Add the following alias:\n",
    "- `unpack` -> `tar xvzf`\n",
    "\n",
    "> We start three node exporters to show a few more things about Prometheus, __one would be enough to monitor your OS!__\n",
    "\n",
    "And let's start the `node_exporter`s (run those inside your command line after downloading, check the comments):\n",
    "- each in separate terminal __OR__\n",
    "- each in the background (using `&` after the command)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# By default it will bind to port 9100, we change it to 8080-8082\r\n",
    "# Run in separate terminals\r\n",
    "# if on Mac, and you've installed the package, just remove the ./ from the commands below before running in the same way\r\n",
    "\r\n",
    "# ./node_exporter --web.listen-address 127.0.0.1:8080\r\n",
    "# ./node_exporter --web.listen-address 127.0.0.1:8081\r\n",
    "# ./node_exporter --web.listen-address 127.0.0.1:8082"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T12:54:43.437243Z",
     "start_time": "2021-04-12T12:54:43.228647Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configure Prometheus to scrape exporters\n",
    "\n",
    "Now that we have our exporters running we need to setup `Prometheus` server to scrape data from them.\n",
    "\n",
    "Let's take a look at the config file (and save it as `prometheus-nodes.yml`):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "---\r\n",
    "global:\r\n",
    "  scrape_interval: '15s'  # By default, scrape targets every 15 seconds.\r\n",
    "  external_labels:\r\n",
    "    monitor: 'codelab-monitor'\r\n",
    "\r\n",
    "scrape_configs:\r\n",
    "  # Prometheus monitoring itself\r\n",
    "  - job_name: 'prometheus'\r\n",
    "    scrape_interval: '10s'\r\n",
    "    static_configs:\r\n",
    "      - targets: ['localhost:9090']\r\n",
    "  # OS monitoring\r\n",
    "  - job_name: 'node'\r\n",
    "    scrape_interval: '5s'\r\n",
    "    static_configs:\r\n",
    "      - targets: ['localhost:8080', 'localhost:8081']\r\n",
    "        labels:\r\n",
    "          group: 'production'\r\n",
    "\r\n",
    "      - targets: ['localhost:8082']\r\n",
    "        labels:\r\n",
    "          group: 'dev'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to give this config file to `Prometheus` server (__it is contained in Docker, remember!__).\n",
    "\n",
    "There are two ways to do that:\n",
    "- Mount directory in your `localhost` to Docker container during runtime: \n",
    "    - When configuration is changing often (and you have autoreload set)\n",
    "- Create new Docker image and copy the configuration:\n",
    "    - When configuration is static and changing rarely\n",
    "    \n",
    "As this configuration will not change often, we will stick the second route (see [here](https://prometheus.io/docs/prometheus/latest/installation/#volumes-bind-mount) for the first approach):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "FROM prom/prometheus\r\n",
    "\r\n",
    "# prometheus.yml is the default path from which Prometheus will take the config\r\n",
    "COPY prometheus-nodes.yml /etc/prometheus/prometheus.yml\r\n",
    "\r\n",
    "# document which ports should be mapped\r\n",
    "EXPOSE 9090"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have this simple `Dockerfile` (named as `prometheus-nodes.Dockerfile`) we have to:\n",
    "- `docker image build` - build the image and tag it \n",
    "- `docker container run` - run Prometheus server container and expose appropriate ports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "docker image build --rm \\\r\n",
    "  --file prometheus-nodes.Dockerfile \\\r\n",
    "  --tag aicore/prometheus-nodes:latest .\r\n",
    "\r\n",
    "docker container run --rm --net=host --publish 9090:9090 aicore/prometheus-nodes:latest                                                 "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Second command might need a little explanation:\n",
    "- We only publish port `9090` as __we want to access data inside Docker container__\n",
    "- Our `node` exporters publish data on `8080`, `8081`, `8082` __BUT on Docker's host machine__\n",
    "- Our `Docker` container (`prometheus` server) __needs access to ports on localhost__ (in order to scrape data)\n",
    "- Hence we have to allow it to communicate with `localhost` via `--net=host` command\n",
    "\n",
    "__Why can't we map ports `8080`, `8081`, `8082`?__\n",
    "\n",
    "Because those are already taken by our exporters (you can only bind one service to one port!)\n",
    "\n",
    "__Go to [`localhost:9090/targets`](http://localhost:9090/targets) to verify everything was set up correctly:__\n",
    "\n",
    "![](images/prometheus_nodes_targets.png)\n",
    "\n",
    "__Go to [`localhost:9090`](http://localhost:9090) and check whether you have `node` commands available:__\n",
    "\n",
    "![](images/prometheus_nodes_queries.png)\n",
    "\n",
    "Run a few of them, you should sometimes (depending on command) see `3` graphs for each node we are monitoring (try `node_cpu_seconds_total` and look for different `localhost:<port>` by hovering over the graph).\n",
    "\n",
    "__In the next lesson, we will learn how to query our data efficiently__, but before that..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise\n",
    "\n",
    "__Your exercise it to set up Prometheus with `3` scrapers about your Operating System:__\n",
    "\n",
    "- Prometheus itself (we did this one previously)\n",
    "- `node` (__rember this one has to be run on `localhost` AND ONLY A SINGLE REPLICA__)\n",
    "- `docker` (we will monitor Docker itself via server... inside `Docker` :D)\n",
    "\n",
    "In order to do that, do the following steps:\n",
    "- Change Docker daemon to enable logging it's metric to port `9323` (check [here](https://docs.docker.com/config/daemon/prometheus/#configure-docker), __only this single bulletpoint!__)\n",
    "- Start a single `node` exporter (by default it exports data to port `9100`)\n",
    "- Create `prometheus.yml` file describing those three services (__each job with a single instance & remember about the ports!__)\n",
    "- Write and build `docker` image containing `prometheus.yml` (like we did previously)\n",
    "- `docker container run` created image (remember about `--net=host` as we have to access data and about mapping containerized `Prometheus` port to `9090`)\n",
    "\n",
    "__Verify targets are being scrapped correctly by checking `localhost:9090/targets`!__\n",
    "\n",
    "### Additionally\n",
    "\n",
    "To make it fully usable for basic system monitoring"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Challenges\n",
    "\n",
    "### Assessment\n",
    "\n",
    "- What are the available alternatives to Prometheus? Read about them [here](https://prometheus.io/docs/introduction/comparison/#). When should we use a different tool? \n",
    "- What is Prometheus's [Push Gateway](https://prometheus.io/docs/practices/pushing/)? When should we use it?\n",
    "- Read a little bit about alerting in Prometheus (e.g. when disaster happens it can send you an e-mail). Go through [documentation](https://prometheus.io/docs/alerting/latest/overview/) and get a basic grasp (leave alerting rules until the next lesson though)\n",
    "\n",
    "### Non-assessment\n",
    "\n",
    "- Local disk storage is limited. How can one integrate with Prometheus's remote storage? Read about it [here](https://prometheus.io/docs/prometheus/latest/storage/).\n",
    "- What is Prometheus Federation? Read about it [here](https://prometheus.io/docs/prometheus/latest/federation/)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash [conda env:.conda-AiCore] *",
   "language": "bash",
   "name": "conda-env-.conda-AiCore-bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}