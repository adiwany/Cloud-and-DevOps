{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data\n",
    "\n",
    "## Learning Objectives\n",
    "- Learn how to identify and visualise missing data\n",
    " - For numerical, categorical and time-series data\n",
    "- The different types of missing values\n",
    "- The appropiate techniques to deal with missing data\n",
    " - Dropping data\n",
    " - Using the mean (but please don't use the mean)\n",
    " - Imputation\n",
    "- Imputing time series and categorical data\n",
    " \n",
    "In the Data Cleaning lesson, we observed that entries of our data were missing. This **missing data** problem occurs with almost every dataset. Data can go missing because of a wide range of reasons. Perhaps the most common reason is a 'faulty' data acquisition process (e.g. defective sensors for measuring temperature data, incomplete patient information etc), however some other reasons could include accidental data deletion or human error.\n",
    "\n",
    "Regardless of how data went missing (well.. not actually regardless - we'll see later on how knowing how data went missing may help our analysis), our job as data scientists is to treat our data based on safe and valid assumptions.\n",
    "\n",
    "Generally speaking, dealing with and treating missing data follows this pipeline:\n",
    "- Identify and convert missing values to null values\n",
    "- Analyse how much data is missing, and the type of missing-ness it is\n",
    "- Either delete the rows with missing values, or impute the missing values\n",
    "\n",
    "In this lesson, I will show examples on the two following datasets: <br>\n",
    "https://archive.ics.uci.edu/ml/datasets/Automobile <br>\n",
    "https://www.kaggle.com/uciml/pima-indians-diabetes-database\n",
    "\n",
    "## Identifying Missing Values\n",
    "One arbitray dataset could present missing values with a variety of differents 'placeholders' for missing values (even over the span of one column!) Examples of common missing values include: `NA`, `-`, `UNKNOWN` etc. The data dictionary/documentation is the first thing you should look at as it may describe how missing values are stored/formatted in the dataset. We should also perform checks by hand - one way to identify missing values is to return the unique values for a column, and sort them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# From the data documentatiobn:\n",
    "names = [\"symboling\", \"normalized-losses\", \"make\", \"fuel-type\", \"aspiration\", \"num-of-doors\", \"body-style\", \"drive-wheels\", \"engine-location\", \"wheel-base\", \"length\", \"width\", \"height\", \"curb-weight\", \"engine-type\", \"num-of-cylinders\", \"engine-size\", \"fuel-system\", \"bore\", \"stroke\", \"compression-ratio\", \"horsepower\", \"peak-rpm\", \"city-mpg\", \"highway-mpg\", \"price\"]\n",
    "auto_df = pd.read_csv(\"../DATA/imports-85.data\", header=None, names=names)\n",
    "auto_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at missing values for stroke\n",
    "np.sort(auto_df[\"stroke\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great - we see a \"?\" which is an indication of a missing value. Let's check a couple of other columns to ensure this value is consistent throughout the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in price:\", np.sort(auto_df[\"price\"].unique()))\n",
    "print(\"Unique values in normalized losses:\", np.sort(auto_df[\"normalized-losses\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question marks in both! Ok, this indicates that this missing value is probably consistent throughout the dataframe (and that more than one type of missing value doesn't exist). From the result of the `.info()` method above, note that although we would expect `stroke` to be of type float, Pandas is indicating to us that it is of type object. This happens because for Pandas, the native missing value is either `np.nan` or `pd.NA`. Now that know what the missing value is - let's reload the data, this time passing the missing value to the `na_values` argument in the `.read_csv()` constructor. The `na_values` flag looks at the string we've provided as the argument, and replaces that string with a `nan` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df = pd.read_csv(\"../DATA/imports-85.data\", header=None, names=names, na_values=\"?\")\n",
    "auto_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that `stroke`, and some other columns have been converted to correct datatypes. \n",
    "\n",
    "NOTE: Historically, Pandas did not support `nan` types for integer numbers - which is why we see some of the above numbers that we would expect to be ints as floats. Recently they have introduced the `Int64` (capital I) type which does have support for a first party null type: `pd.NA`. You can read more here: https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html\n",
    "\n",
    "We'll now be working with a diabetes dataset known as Pima. This was a study about diabetes on a Native American group of people known as Pima people. Let's load in the dataset and see if we can identify null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_df = pd.read_csv(\"../DATA/datasets_228_482_diabetes.csv\")\n",
    "print(pima_df.info())\n",
    "pima_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular dataset is interesting because sometimes missing values aren't as obvious as having an explicit value. If we return the describe method, we might be able to find some questionable summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there any *row* which particularly stands out to you as questionable?\n",
    "\n",
    "\n",
    "The `min` row is particularly interesting because many of the values there are 0. Do you know any alive person who has a `BloodPressure` of 0? Or `Glucose`, `SkinThickness`, `Insulin` or `BMI` for that matter. It is obviously possible to have 0 `Pregnancies` though, so that value of 0 could be considered correct. For the questionable columns we identified, let's check how many 0 values are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionable_columns = [\"BloodPressure\", \"Glucose\", \"SkinThickness\", \"Insulin\", \"BMI\"]\n",
    "zero_counts = {col: 0 for col in questionable_columns}\n",
    "for col in questionable_columns:\n",
    "    zero_counts[col] = pima_df[col][pima_df[col] == 0].count()\n",
    "    \n",
    "print(zero_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had performed visualisation on our data, identifying this would have been easier. Let's plot a histogram (which will be formally introduced later) and use `BloodPressure` as an example. Note that Plotly Express handles a LOT of things under the hood - and 'binning' float values is one of these things. We need to keep such things in mind when using high level libraries as otherwise we may make incorrect assumptions about our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.histogram(pima_df, \"BloodPressure\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we can't pass \"0\" as an argument to `na_values` `.read_csv()` like we did previously because some of our 0 values are legitimate. Instead, we'll have to 'manually' replace these values with `np.nan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace 0 with np.nan for the questionable columns\n",
    "\n",
    "\n",
    "## describe the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The amount of missingness\n",
    "It can be valuable to know how much of our data is actually missing - in terms of absolute values or percentages. Doing so is a relatively straightforward process which I will demonstrate on our `auto_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df_null = auto_df.isnull() # .isna() is the same as .isnull()\n",
    "auto_df_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now obtained a dataframe with True/False values - True indicating where there is a missing/null value, and False otherwise. To see the absolute amount of missing values, we can `.sum()` the dataframe. To obtain the percentages, we can simply do a `.mean() * 100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df_null.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using one line only, work out the total percentage of missing values from the pima dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Missing Data\n",
    "There is a very useful package called `missingno` which allows us to easily visualise our data and identify rows where our data is missing. Doing this allows you to graphically visualise which rows have missing data, and can help us to determine whether data has gone missing because of a random error or because of something a bit more systematic.\n",
    "\n",
    "For example, in the auto_df plot, we see that where "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "\n",
    "msno.matrix(auto_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msno.bar(pima_df)\n",
    "msno.matrix(pima_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Missing Data\n",
    "We conceptually touched on this earlier, but there exists a three level taxonomy for types of missing data:\n",
    "\n",
    "1. **Missing Completely at Random (MCAR)**: implies that the data goes missing at a consistent rate independently of other variables. This is conceptually simple to think about - in the PIMA dataset example above, the Glucose readings were mostly likely due to a faulty piece of equipment failing to measure the `Glucose` level correctly a certain percentage of the time. \n",
    "\n",
    "2. **Missing at Random (MAR)**: implies that the data goes missing at a consistent rate, but is dependent on other variables and must be determined through other pieces of information from our data. A typical example for MAR is that of studying the relation between salary and IQ. If above average individuals were more likely to skip the \"what's your salary\" question, results would obviously be skewed and analysts which didn't take the MAR into account would fail to find the expected positive association between IQ and salary. `BloodPressure` in the above plot may be considered to be type MAR.\n",
    "\n",
    "3. **Not Missing at Random (NMAR/MNAR)**: implies the reason the data goes missing because of the variable itself. It is rarer than MAR and harder to identify, but the treatment is the same. An example could be that, in a depression survey, let's say that many respondants answer to the question \"are you depressed\" are missing. If this data is missing because respondants who have depression aren't answering survey, you have missing data of type MNAR. A problem with identifying this type of missingness is that it is impossible to verify that it is the case without actually knowing the missing values. If a substantial amount of data is missing, however, we may be able to assume something slightly more systematic (as is the case with `Insulin` below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(pima_df.sort_values(\"Insulin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To more easily identify how related the missingness of variable is because of another, we can use a heatmap or dendrogram. In the heatmap example below, we see that there is a 0.7 correlation between missing data in `Insulin` and `SkinThickness`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.heatmap(pima_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alternative is the dendrogram - which is a diagram that can visualise heirarchical relationships between groups/objects. Reading a dendrogram can be a bit confusing the first time you come across it - but the key to understanding them is to look at the \"level\" they are joined on. The smaller the depth of the connection, the more similar those two variables are (in terms of missingness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.dendrogram(pima_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When (and how) to delete data\n",
    "\n",
    "When we deem missing data from one of our variables to MCAR, a valid solution could be to delete the data for which we have missing values. Note that we should only delete data we believe to MCAR - not MAR or MNAR, There are two types of deletion we can consider:\n",
    "1. Pairwise Deletion\n",
    "2. Listwise Deletion\n",
    "\n",
    "**Pairwise deletion** is when missing values are skipped during the calculation of some statistic. This is almost a non-factor with Pandas because when we compute a statistic, missing values aren't considered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean of Glucose using native mean method:\", pima_df[\"Glucose\"].mean())\n",
    "print(\"Mean of Glucose via manual calculation  :\", pima_df[\"Glucose\"].sum() / pima_df[\"Glucose\"].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Listwise deletion** is when we drop the whole row of data because of a missing value. This is completely valid to do but should only be done when the amount of rows you are dropping is insignificant when compared to the rest of the dataset. From our counts of nullity that we did previously, and working under the knowledge that `Glucose` and `BMI` are MCAR, we are safe to drop the rows where these missing values are present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the rows in Glucose and BMI for which there are missing values\n",
    "\n",
    "\n",
    "## Plot a missingno matrix of the dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing using averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imputation** is the act of predicting the missing data, and can be applied to any of the missing data classifications we are working with. In this section, we will look at imputing data using central tendancy measures. We will show why this may not be a great idea, and then introduce ML imputing techniques. Recall the three types of average: Mean, Median and Mode.\n",
    "\n",
    "The values of these types of imputations are trivial to calculate and we could easily do it ourselves if we wanted to. However, for the sake of introducing you to the API, we will be using skleanr's `SimpleImputer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_cols = pima_df.columns\n",
    "pima_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "mode_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "pima_mode_arr = mode_imputer.fit_transform(pima_df)\n",
    "pima_mode_df = pd.DataFrame(data=pima_mode_arr, columns=pima_cols)\n",
    "pima_mode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Impute and assign dataframes for strategies of mean and median\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So why is this bad?\n",
    "\n",
    "Central tendancy imputations should be avoided because they reduce the variance of the data, leading to a higher bias in the data. Perhaps more intuitively, these types of imputations will not consider any of the other variable relationships in your data. We can easily see this with visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = (pima_df[\"SkinThickness\"].isnull() + pima_df[\"BMI\"].isnull()).astype(\"int\")\n",
    "px.scatter(pima_mean_df, x=\"SkinThickness\", y=\"BMI\", title=\"SkinThickness vs BMI (Mean Imputation)\",\n",
    "           color=nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the different imputations through subplots\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig_mean = px.scatter(pima_mean_df, x=\"SkinThickness\", y=\"BMI\", color=nulls)\n",
    "\n",
    "fig_median = px.scatter(pima_median_df, x=\"SkinThickness\", y=\"BMI\", color=nulls)\n",
    "\n",
    "fig_mode = px.scatter(pima_mode_df, x=\"SkinThickness\", y=\"BMI\", color=nulls)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=3, shared_xaxes=False, subplot_titles=(\"Mean Imputation\",\"Median Imputation\", \"Mode Imputation\"))\n",
    "fig.add_trace(fig_mean['data'][0], row=1, col=1)\n",
    "fig.add_trace(fig_median['data'][0], row=1, col=2)\n",
    "fig.add_trace(fig_mode['data'][0], row=1, col=3)\n",
    "fig.update_layout(title=\"SkinThickness vs BMI (Imputations)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML based imputation techniques\n",
    "\n",
    "The alternative to using central tendencies or constant values for imputations is to use ML based imputation methods. We will cover three types algorithms here which have popular use: Nearest neighbours imputation, tree based imputation and regression based imputation. These methods work by building models for a feature based on the other features of the data.\n",
    "\n",
    "As you've already covered the underlying algorithms of KNNs, Ensemble Trees and Regression, we won't recap them here - just show how to apply them to our dataset.\n",
    "\n",
    "We'll start with KNN imputation. Here, the algorithm selects the K nearest/most similar datapoints to a datapoint with a missing value. The missing value is then either populated with an average from the K neighbours, or a weighted average. This argument can be specified by `weights` flag in sklearn's [`KNNImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer) class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# default arguments are:\n",
    "# n_neighbors = 5\n",
    "# weights = \"uniform\"\n",
    "knn_impute = KNNImputer(n_neighbors=3, weights=\"distance\")\n",
    "pima_knn_arr = knn_impute.fit_transform(pima_df)\n",
    "pima_knn_df = pd.DataFrame(data=pima_knn_arr, columns=pima_cols)\n",
    "\n",
    "fig_knn = px.scatter(pima_knn_df, x=\"SkinThickness\", y=\"BMI\", title=\"SkinThickness vs BMI (KNN Imputation)\",\n",
    "           color=nulls)\n",
    "fig_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When researching imputation techniques, one common method you'll come across is something known as [**MICE**](https://www.jstatsoft.org/article/view/v045i03) - Multivariate Imputation by Chained Equations. This algorithm performs multiple regressions of a random sample of data, and uses the average of these multiple regressions to impute the missing value. With the sklearn API, the appropiate method to use is the [`IterativeImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html) class, passing `sample_posterior=True` ([source](https://scikit-learn.org/stable/modules/impute.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IterativeImputer is an experimental feature in sklearn, so we need to enable it prior to using it\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "regression_impute = IterativeImputer(sample_posterior=True)\n",
    "pima_regression_arr = regression_impute.fit_transform(pima_df)\n",
    "pima_regression_df = pd.DataFrame(data=pima_regression_arr, columns=pima_cols)\n",
    "\n",
    "fig_br = px.scatter(pima_regression_df, x=\"SkinThickness\", y=\"BMI\", title=\"SkinThickness vs BMI (Regression Imputation)\",\n",
    "           color=nulls)\n",
    "fig_br"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `IterativeImputer` class is highly flexible and allows us to use any estimator object to perform our imputation (instead of just regression). The default estimator that it uses isn't actually vanilla linear regression - it's something known as Bayesian Ridge regression. We won't cover the details here as the important thing to know is that it is just a linear regression variant. If you are curious about a fuller understanding, more details can be found at: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html#sklearn.linear_model.BayesianRidge.\n",
    "\n",
    "Another, more recent, trend in imputation is using tree based ensembles. Here we will use the `RandomForestRegressor` estimator to impute the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "## Impute the missing values using RandomForestRegressor and IterativeImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=3, shared_xaxes=False, subplot_titles=(\"KNN Imputation\", \"BR Regression Imputation\", \"RF Regression Imputation\"))\n",
    "fig.add_trace(fig_knn['data'][0], row=1, col=1)\n",
    "fig.add_trace(fig_br['data'][0], row=1, col=2)\n",
    "fig.add_trace(fig_rf['data'][0], row=1, col=3)\n",
    "fig.update_layout(title=\"SkinThickness vs BMI (Regression Imputations)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Imputation\n",
    "\n",
    "Time series data referes to data that has been collected over time, with each datapoint being indexed by a sequential datetime. The most typical example is perhaps stock data. Here, we will look at the Beijing PM2.5 Data available from: https://www.kaggle.com/joshuapaulbarnard/beijing-air-quality-pm25-from-2010-to-2017?select=Beijing+PM2_5+from+2010+to+2017.csv. In the `.read_csv()` method, we'll tell Pandas that we want to index on the Date variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_df_org = pd.read_csv(\"../DATA/Beijing PM2_5 from 2010 to 2017.csv\", index_col=\"Date\", parse_dates=True, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_df = pm_df_org[:400000]\n",
    "pm_df = pm_df.loc[~pm_df.index.duplicated(keep='last')]\n",
    "pm_df = pm_df.sort_index(axis=0)\n",
    "pm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the percentage of nulls in each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns so they're easier to refer to\n",
    "pm_cols = [\"city\", \"country\", \"season\", \"pm25\", \"dew_point\", \"temperature\", \"humidity\", \"pressure\", \"wind_direction\", \"wind_speed\", \"precipitation_hourly\", \"preciptiation_cum\"]\n",
    "pm_df.columns = pm_cols\n",
    "pm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill missing values\n",
    "\n",
    "We will start with the [`.fillna()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html) method. The `.fillna()` method does what the name implies - it fills values where there is a NaN. This method has two strategies we can adopt:\n",
    "- `ffill`\n",
    "- `bfill`\n",
    "\n",
    "Which stand for \"forward fill\" and \"backward fill\" respectively. If we have a Series with some missing values in it, `ffill` will replace the NaNs with the last non-NaN value we've come across in the Series - until the next non-NaN value is reached. Backward fill reverses this and fills in the non-NaN values with the *next* non-NaN value we would observe. Seeing this in action will clarify this description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_df[\"wind_speed\"][31230:31245]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_ffill_df = pm_df.fillna(method=\"ffill\")\n",
    "pm_ffill_df[\"wind_speed\"][31230:31245]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_bfill_df = pm_df.fillna(method=\"bfill\")\n",
    "pm_bfill_df[\"wind_speed\"][31230:31245]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation\n",
    "\n",
    "Ok - so this works to fill NA values, but it's not really ideal. (Note that the fillna method has applications outside of time series data). The next level up is the [`.interpolation()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html) function. Interpolation is art of finding a transition from one point to the other. `.interpolation()` has many useful techniques to interpolate datapoints, but we will focus on the following 3. Refer to the documentation for a more comprehensive list, and the recommended place for when to use the method:\n",
    "- Linear\n",
    "- Quadratic\n",
    "- Nearest\n",
    "\n",
    "#### Linear\n",
    "Linear interpolation simply fills values with equidistant points from the first non-NaN value to the next. Visually:\n",
    "![](../images/linear_interpolation.png)\n",
    "\n",
    "It would be trivial to calculate what this 'equidistant' value actually is - we take the first non-NaN value and the following non-NaN value, subtract the two together, and divide it by the number of NaNs between the two points. With the `wind_speed` data we were looking at, this would be $(2.4 - 1.3)/6 = 0.183$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_linear_df = pm_df.interpolate(method=\"linear\")\n",
    "pm_linear_df[\"wind_speed\"][31230:31245]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nearest\n",
    "\n",
    "The `nearest` strategy is very similar to that of `.fillna()` in that it fills the values. However, this method simply fills in the NaNs with the closest non-NaN value to the current NaN  we're looping over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pm_nearest_df = pm_df.interpolate(method=\"nearest\")\n",
    "pm_nearest_df[\"wind_speed\"][31230:31245]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quadratic\n",
    "Quadratic interpolation attempts to fit a quadratic/parabolic curve between the two non-NaN values. Later in this notebook we will show plots of the different interpolation methods and you'll be able to get a better sense of understanding how and where each of the interpolation methods we are presenting would be useful. We won't be diving into the methodology behind quadratic interpolation, but you can find a brilliant walkthrough here: https://www.youtube.com/watch?v=ifS8LL3qT2g\n",
    "\n",
    "![](../images/quadratic_interpolation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_quad_df = pm_df.interpolate(method=\"quadratic\")\n",
    "pm_quad_df[\"wind_speed\"][31230:31245]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Time-series data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_df[\"wind_speed\"][31230:31245]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = 'plotly'\n",
    "\n",
    "fig = make_subplots(rows=6, cols=1, shared_xaxes=False, subplot_titles=(\"None\", \"Ffill Interpoloation\", \"Bfill Interpoloation\", \"Linear Interpoloation\", \"Nearest Interpoloation\", \"Quadratic Interpoloation\"))\n",
    "\n",
    "default = pm_df[\"wind_speed\"][31230:31245]\n",
    "\n",
    "fig_none = default.plot()\n",
    "fig.add_trace(fig_none[\"data\"][0], row=1, col=1)\n",
    "\n",
    "figs_list = [pm_ffill_df, pm_bfill_df, pm_linear_df, pm_nearest_df, pm_quad_df]\n",
    "for i, to_fig in enumerate(figs_list):\n",
    "    fig_ = to_fig[\"wind_speed\"][31230:31245].plot(color_discrete_sequence=[\"red\"])\n",
    "    fig_.add_trace(px.line(pm_df[\"wind_speed\"][31230:31245]).data[0])\n",
    "    \n",
    "    fig.add_trace(fig_[\"data\"][0], row=i+2, col=1)\n",
    "    fig.add_trace(fig_[\"data\"][1], row=i+2, col=1)\n",
    "\n",
    "fig.update_layout(title=\"Wind Speed Interpolations\", width=np.inf, height=1600, showlegend=False)\n",
    "fig.update_traces(mode='markers+lines')\n",
    "fig.show()\n",
    "\n",
    "# fig = pm_df.interpolate(method=\"quadratic\")[\"wind_speed\"][31230:31245].plot()\n",
    "# fig.add_trace(px.line(pm_df[\"wind_speed\"][31230:31245], color_discrete_sequence=[\"red\"]).data[0])\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Categorical Variables\n",
    "A naive approach to impute categorical data is to either use the most frequent/mode method or the `.fillna()` strategy that we looked at earlier. However, it is possible to impute categorical variables as we would a typical continuous variable. We cannot directly perform the imputation as straightforwardly as before because the categorical variables are usually encoded as strings (and therefore we can not perform any mathematical operations on them). To impute these kinds of variables, we must first encode them them as numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "knn_impute = KNNImputer(n_neighbors=3, weights=\"distance\")\n",
    "\n",
    "# We will drop city and country because these are categorical variables which have all their values present\n",
    "# We will keep season even though it has all the values present (just to demonstrate the process over multiple categorical variables)\n",
    "# We will drop humidity and precipitation_cum as these features are fully NaN\n",
    "pm_to_impute_cols = [\"season\", \"pm25\", \"dew_point\", \"temperature\", \"pressure\", \"wind_direction\", \"wind_speed\", \"precipitation_hourly\"]\n",
    "pm_to_impute_df = pm_df[pm_to_impute_cols]\n",
    "cat_cols = [\"season\", \"wind_direction\"]\n",
    "\n",
    "# key: col, value: list of tuples\n",
    "category_dict_encode = {}\n",
    "category_dict_decode = {}\n",
    "for col in cat_cols:\n",
    "    pm_to_impute_df[col] = pm_to_impute_df[col].astype(\"category\")\n",
    "    categories = pm_to_impute_df[col].cat.categories\n",
    "    \n",
    "    col_cat_dict = dict(enumerate(categories))\n",
    "    print(\"Column, Category dict:\", col_cat_dict)\n",
    "    category_dict_decode[col] = col_cat_dict\n",
    "    \n",
    "    col_cat_dict = {v:k for k,v in col_cat_dict.items()}\n",
    "    print(\"Inverted Column, Category dict:\", col_cat_dict)\n",
    "    category_dict_encode[col] = col_cat_dict\n",
    "\n",
    "print()\n",
    "print(\"Category dict ENCODE:\", category_dict_encode)\n",
    "print(\"Category dict DECODE:\", category_dict_decode)\n",
    "pm_to_impute_df.replace(category_dict_encode, inplace=True)\n",
    "pm_to_impute_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pm_arr = knn_impute.fit_transform(pm_to_impute_df)\n",
    "knn_pm_df = pd.DataFrame(knn_pm_arr, columns = pm_to_impute_cols)\n",
    "knn_pm_df[31230:31245]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    knn_pm_df[col] = knn_pm_df[col].round()\n",
    "knn_pm_df.index = pm_df.index\n",
    "knn_pm_df[31230:31245]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the original dataframe we will only replace the values for the categorical cols we imputed\n",
    "# We need to map the columns back to their codes\n",
    "knn_pm_df.replace(category_dict_decode, inplace=True)\n",
    "knn_pm_df[31230:31245]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_cols_df = knn_pm_df[cat_cols]\n",
    "imputed_cols_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pm_imputed_df = pm_df.copy(deep=True)\n",
    "pm_imputed_df.update(knn_pm_df, overwrite=False)\n",
    "pm_imputed_df[31230:31245]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nulls = pm_df[\"wind_speed\"][31230:31245].isnull()\n",
    "print(nulls)\n",
    "\n",
    "fig = pm_imputed_df[\"wind_speed\"][31230:31245].plot(color_discrete_sequence=[\"red\"])\n",
    "fig.add_trace(px.line(pm_df[\"wind_speed\"][31230:31245]).data[0])\n",
    "fig.update_traces(mode='markers+lines')\n",
    "fig.update_layout(title=\"Wind Speed Interpolation (KNN Full DF imputation)\")\n",
    "fig[\"data\"][0][\"name\"] = \"Interpolated\"\n",
    "fig[\"data\"][1][\"name\"] = \"Orignal\"\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
