{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e5dd8a",
   "metadata": {},
   "source": [
    "# Kubernetes Basics\n",
    "\n",
    "> Open Source platform managing __containerized__ workloads __focused on declarative configuration and automation__\n",
    "\n",
    "What Kubernetes allows us to do:\n",
    "- Container deployment across clusters:\n",
    "    - Which and how many containers should be deployed\n",
    "    - How many replicas for fault tolerance\n",
    "    - __Self-healing__ - if a container is down (not responding to health checks) __spin a new one__\n",
    "    - __Auto-scaling__ - given increased/decreased requirements __spin up or destroy containers in the cluster__\n",
    "- __Discoverability__ - each container can be accessed via it's name (or IP) and port\n",
    "- __Load Balancing__ - traffic to the cluster might be distributed if one node is overloaded\n",
    "- Automated rollouts and rollbacks - describe __the desired state__ and `k8s` tries to match it automatically\n",
    "- Storage orchestration - allows us to mount storage (per-node, shared and others) to save/read data\n",
    "\n",
    "> __Kubernetes is a \"low level\" platform, A TOP OF WHICH other projects reside; consider it \"a programming language for deployment\"__ \n",
    "\n",
    "## Containers == game changer\n",
    "\n",
    "Let's see how containers changed the deployment landscape:\n",
    "\n",
    "![](./images/container_evolution.svg)\n",
    "\n",
    "__Benefits__:\n",
    "- More lightweight than VMs\n",
    "- Immutable\n",
    "- Easy to create/destroy when needed\n",
    "- __Developers can pack the code with all necessities__; ops take care of deploying \"described black boxes\" \n",
    "- Reproducible - runs the same everywhere (more than a VM)\n",
    "- __Micro-services__ - app can be broken into multiple separate containers communicating with each other:\n",
    "    - We only change image we need\n",
    "    - No need to deploy everything a-new\n",
    "    - Easily switchable components \n",
    "\n",
    "## Imperative vs Declarative\n",
    "\n",
    "> Using imperative programming (or configuration) __we define each necessary step to get to the result__\n",
    "\n",
    "Some examples could be:\n",
    "- Create variable `x` and assign value `1` to it\n",
    "- Scrape data from the website and get appropriate `<tag>`\n",
    "- Pass data `X` through linear layer and apply sigmoid on top of it\n",
    "\n",
    "In contrast, __declarative programming__:\n",
    "\n",
    "> Using declarative programming (or configuration) __we describe the desired state of system without describing the steps__\n",
    "\n",
    "Respectively for the examples above:\n",
    "- I want variable `x` with value `1` assigned to it\n",
    "- I want `<tag>` out of this website\n",
    "- I want binary probabilities prediction for data `X`\n",
    "\n",
    "> #### Kubernetes uses declarative programming to describe state of the whole system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd02379",
   "metadata": {},
   "source": [
    "# Kubernetes Components\n",
    "\n",
    "> Kubernetes has __a lot__ of concepts one should be familiar with in order to work with the platform efficiently\n",
    "\n",
    "When we __deploy Kubernetes__ we obtain a cluster with the components shown in the diagram below:\n",
    "\n",
    "![](./images/components-of-kubernetes.svg)\n",
    "\n",
    "## High level components overview\n",
    "\n",
    "- __`Node`s__ - worker machines (__either physical or virtual__) which host containerized application\n",
    "- __`POD`s__ - component of the application, __smallest deployable unit of work in `k8s`__:\n",
    "    - think of it as our program running within `Node`\n",
    "    - consists of a few containers which create applicaiton together\n",
    "    - is bundled with `storage`, `network` address etc. to create a coherent deployment unit\n",
    "- __Control Plane__ - manages `node`s and `POD`s\n",
    "\n",
    "> For learning (at the beginning) we will have a single `Node` on the same physical machine\n",
    "\n",
    "> In production environment __`Control Plane`__ can span multiple machines for increased fault tolerance\n",
    "\n",
    "> In production environment we usually have multiple `Node`s (up to thousands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27ba8ef",
   "metadata": {},
   "source": [
    "## Control Plane Components\n",
    "\n",
    "### kube-apiserver\n",
    "\n",
    "> __Exposes `Kubernetes` API using which we can communicate with the cluster__\n",
    "\n",
    "Communication with the cluster can be done via:\n",
    "- `http` requests as `kube-apiserver` provides REST server (__read more about it [here](https://kubernetes.io/docs/concepts/overview/kubernetes-api/)__)\n",
    "- __through command line__:\n",
    "    - `kubectl` - control kubernetes cluster and workload\n",
    "    - `kubeadm` - bootstraps kubernetes clusters (initializing it from config, tearing down etc.)\n",
    "      \n",
    "> __We should almost exclusively use `command line` tools to interact with `kube-apiserver`!__\n",
    "\n",
    "Reasons are:\n",
    "- increased readability\n",
    "- easier automation\n",
    "- `cli` tools handle different internal API version of `k8s` for us\n",
    "\n",
    "### etcd\n",
    "\n",
    "> __`etcd` stores Kubernetes Objects which define the whole cluster__\n",
    "\n",
    "Features:\n",
    "- Data is stored locally on control plane node(s)\n",
    "- __In case all of our control planes go down we lose cluster state__ (and we have to recreate the whole infrastructure a-new)\n",
    "\n",
    "> `etcd` data should be backed in an external storage, check [here](https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster) for more information\n",
    "\n",
    "### kube-scheduler\n",
    "\n",
    "> __Watched newly created `POD`s and assigns them to `node`__\n",
    "\n",
    "How is the node chosen?\n",
    "- Resource requirements for our `POD`\n",
    "- Applied policies/constraints\n",
    "- Data locality (__move code towards data `spark`__)\n",
    "\n",
    "If you are curious about scheduling PODs in more detail check out [this part of documentation](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)\n",
    "\n",
    "### kube-controller-manager\n",
    "\n",
    "> Runs controller processes\n",
    "\n",
    "What is a `control process`?\n",
    "\n",
    "> Loop which watches the cluster (through `apiserver`) and __attempts to move the current state to the desired one__\n",
    "\n",
    "Each controller runs in a separate process, one example could be a `node controller` which responds when node goes down.\n",
    "\n",
    "### cloud-controller-manager\n",
    "\n",
    "> Similar to `kube-controller-manager` __but watches state of cloud resources__\n",
    "\n",
    "For example `node controller` queries cloud provider for node health check. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7954a16b",
   "metadata": {},
   "source": [
    "## Node Components\n",
    "\n",
    "Each node consists of the following:\n",
    "\n",
    "### kubelet\n",
    "\n",
    "> Given `PodSpec` (specification we will later see) __ensures appropriate containers run within `POD` and are healthy__\n",
    "\n",
    "### kube-proxy\n",
    "\n",
    "> Governs networks rules on nodes. __Enables communication to/from `POD`s within and outside cluster__\n",
    "\n",
    "### Container runtime\n",
    "\n",
    "> The container runtime is the software that is responsible for running containers\n",
    "\n",
    "Some possibilities include:\n",
    "- [`Docker`](https://docs.docker.com/engine/) (which we already know)\n",
    "- [`containerd`](https://containerd.io/docs/) - simple container runtime (alternative to `Docker`)\n",
    "\n",
    "Any project which fulfills [Kubernetes CRI specification](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f8ea3",
   "metadata": {},
   "source": [
    "# Kubernetes Objects\n",
    "\n",
    "> __Persistent entities in `k8s` system which represent OUR DESIRED STATE of cluster__\n",
    "\n",
    "Using objects we can describe:\n",
    "- What applications and where (which node) these are running\n",
    "- How applications behave (e.g. when to restart, delete, upgrade)\n",
    "- Resources available to applications\n",
    "\n",
    "In order to create them __we have to request `kubelet-apiserver`'s API__!\n",
    "\n",
    "As mentioned previously, instead of directly sending REST requests we can (and will) use `kubectl` for that.\n",
    "\n",
    "> __To specify how we want `k8s` to run we have to describe objects__\n",
    "\n",
    "> Each `k8s` object has a unique ID assigned by the platform\n",
    "\n",
    "## Describing Kubernetes objects\n",
    "\n",
    "As API is RESTful we can include `JSON` description of object.\n",
    "\n",
    "__This way is supported, although hard to maintain__\n",
    "\n",
    "> __What we can do instead is use more readable `YAML` to specify our objects and let `kubectl` transform the representation to `JSON`__\n",
    "\n",
    "Let's see an example object configuration and describe the fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "242c2c46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T21:26:18.283113Z",
     "start_time": "2021-08-17T21:26:17.145589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash: ---: command not found\n",
      "bash: apiVersion:: command not found\n",
      "bash: kind:: command not found\n",
      "bash: metadata:: command not found\n",
      "bash: name:: command not found\n",
      "bash: spec:: command not found\n",
      "bash: selector:: command not found\n",
      "bash: matchLabels:: command not found\n",
      "bash: app:: command not found\n",
      "bash: replicas:: command not found\n",
      "bash: template:: command not found\n",
      "bash: metadata:: command not found\n",
      "bash: labels:: command not found\n",
      "bash: app:: command not found\n",
      "bash: spec:: command not found\n",
      "bash: containers:: command not found\n",
      "bash: -: command not found\n",
      "bash: image:: command not found\n",
      "bash: ports:: command not found\n",
      "bash: -: command not found\n",
      "\u001b[?2004h"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: nginx-deployment\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: nginx\n",
    "  replicas: 2 # tells deployment to run 2 pods matching the template\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: nginx\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: nginx\n",
    "        image: nginx:1.14.2\n",
    "        ports:\n",
    "        - containerPort: 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069f62e3",
   "metadata": {},
   "source": [
    "- `apiVersion` - specifies `k8s` API version (`kubectl` will use it to make appropriate requests)\n",
    "- `kind` - Kind of object we want to create (we will talk about it later during __Workloads__)\n",
    "- `metadata` - Uniquely defines the object, usually using `name` field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52354fba",
   "metadata": {},
   "source": [
    "## spec and... status\n",
    "\n",
    "> __`spec` describes DESIRED characteristics of our resource__\n",
    "\n",
    "This one is provided by user, and, in this case, tells the `kubernetes` platform that:\n",
    "- Every POD which has a label `app` with name `nginx` is part of `Deployment` kind (__described later__)\n",
    "- Two `POD`s should be run on the cluster (`replicas`)\n",
    "- Each one should be created from a `template` which:\n",
    "    - Gives `app` label with a value `nginx`\n",
    "    - `spec`ifies it is created from a single container which will be named `nginx` (for discoverability)\n",
    "    - Uses `image` `nginx:1.14.2`\n",
    "    - And has container port `80` open for communication\n",
    "    \n",
    "Okay, where is `status` though?\n",
    "\n",
    "> __`status` describes CURRENT state of the resource__\n",
    "\n",
    "__While we provide `spec`, `status` is updated internally by `kubernetes`__\n",
    "\n",
    "Providing `status` has the following benefits:\n",
    "- Allows users to query the current status of the deployment\n",
    "- __Allows controllers to act based on the current `status`__ and drive the objects toward the desired state\n",
    "\n",
    "### Desired vs Actual state\n",
    "\n",
    "A little digression:\n",
    "\n",
    "> __Do not worry about `actual` state, because our claster might never reach it !__\n",
    "\n",
    "As long as `controlers` are continuously running everything is fine.\n",
    "\n",
    "Descrepancy beetwen current and desired state might happen due to plethora of reasons, e.g: \n",
    "\n",
    "- Freaquent updates of `config` files (`kubernetes` had no time to act accordingly to our changes)\n",
    "- We are handling hundreds of nodes and our change hasn't being propagate yet\n",
    "- Random node failures (`kubernetes` will reinstantiate necessary `POD`s) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb58d9d9",
   "metadata": {},
   "source": [
    "## Creating and managing objects\n",
    "\n",
    "Now that we have our object described we should communicate to `kube-apiserver` we want it created.\n",
    "\n",
    "There are `3` techniques to do that:\n",
    "- __Imperative commands__ - operates on existing objects __via commands__ (discouraged!)\n",
    "- __Imperative object configuration__ - operate on individual files\n",
    "- __Declarative object configuration__ - works on directories of files\n",
    "\n",
    "> __Approaches should not be mixed together as it may result in `undefined behavior`!__\n",
    "\n",
    "__We will mostly focus on `declarative` approach__\n",
    "\n",
    "### Imperative object configuration\n",
    "\n",
    "> __`kubectl` specifies operation to carry given a `.yaml` config file__\n",
    "\n",
    "Some examples:\n",
    "\n",
    "> Create the objects defined in a configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57c648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl create -f nginx.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c829297a",
   "metadata": {},
   "source": [
    "> Delete the objects defined in two configuration files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c65c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl delete -f nginx.yaml -f redis.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16706684",
   "metadata": {},
   "source": [
    "> Update the objects defined in a configuration file by overwriting the live configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd40a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl replace -f nginx.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd3ff13",
   "metadata": {},
   "source": [
    "### Declarative object configuration\n",
    "\n",
    "> Specify configuration files, __but do not specify the operation which should be performed__\n",
    "\n",
    "`create`, `update`, `delete` operations are __automatically picked up__ by Kubernetes engine.\n",
    "\n",
    "Pros of this approach:\n",
    "- __We can easily work on directories of `config` files__ - all of them will be `applied` appropriately\n",
    "- __Different configs allow us to handle more objects__\n",
    "- __Only two commands needed__\n",
    "- Does not replace the whole configuration \"a-new\", only patches it\n",
    "\n",
    "To check which operations will be applied we can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279f202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl diff -f ./configs\n",
    "\n",
    "# Recursively find all `.yaml` files\n",
    "\n",
    "kubectl diff -R -f ./configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724bf39",
   "metadata": {},
   "source": [
    "Once we verify that parsed configs should bring cluster into the desired state one can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e1528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -R -f ./configs # R for recusrive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9083d4",
   "metadata": {},
   "source": [
    "Unfortunately:\n",
    "- Might be harder to debug (especially when starting out with `k8s`)\n",
    "- Complexity of operations hidden under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361cc71d",
   "metadata": {},
   "source": [
    "# Workloads\n",
    "\n",
    "> Workload is an application running on `kubernetes`\n",
    "\n",
    "Each `workload` runs within __a set of `POD`s__, while `POD` __represents a set of containers running together__.\n",
    "\n",
    "- `POD`s are scheduled to run on nodes\n",
    "- In case of `node` failure all of the `POD`s on the node are also deleted (__`POD`s are ephemeral, should be easy to (re)create__)\n",
    "- If `POD` run fails it will stay in inappropriate state (more details in `POD` section)\n",
    "\n",
    "As there are a lot of failure points cluster admin would have to:\n",
    "- Verify whether `POD`s are running fine constantly\n",
    "- Manually reschedule `POD`s to different `Node`s\n",
    "- Manually restart `POD`s in case any container failed\n",
    "\n",
    "> __Manual handling is pretty inefficient, hence `Workload Resources` were created__\n",
    "\n",
    "`Workload Resources` can specify (amongst other things):\n",
    "- When `POD` should be recreated\n",
    "- What to do in case of failures\n",
    "- How many times should we try to reschedule before giving up \n",
    "\n",
    "> ## `Workload Resources` should be used to manage `POD`s lifecycles, AVOID DEPLOYING \"BARE PODS\"!\n",
    "\n",
    "But before that we should know a little more about `POD`s themselves..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f501e67",
   "metadata": {},
   "source": [
    "# PODs\n",
    "\n",
    "> __A group of one or more containers, with shared context and a specification for how to run the containers__\n",
    "\n",
    "PODs add another layer of abstraction over the containers and behave similarly to `docker-compose`, e.g. __they can connect multiple related containers into one logical grouping__.\n",
    "\n",
    "Shared context:\n",
    "- Shared storage\n",
    "- Shared network resources (e.g. IP)\n",
    "- Linux namespaces\n",
    "\n",
    "__Individual applications might be further isolated within the `POD`__\n",
    "\n",
    "> `POD` is minimal deployment unit in Kubernetes\n",
    "\n",
    "This means we cannot schedule containers on their own.\n",
    "\n",
    "## Lifecycle\n",
    "\n",
    "> `POD`s remain on scheduled `Node` until termination (according to restart `policy` if failure occured) or deletion\n",
    "\n",
    "__`POD`s are never moved across nodes, they are eventually recreated!__\n",
    "\n",
    "Some features:\n",
    "- `POD`s cannot self-heal (e.g. restsart themselves). This is done by appropriate `Workload Resources` or by cluster admin\n",
    "- __PODs can restart failed containers though__ (using `kubelet`)\n",
    "- Related resources (e.g. `volume`s) are also deleted after `POD` termination (unless specified otherwise)\n",
    "\n",
    "`POD`s can be in one of multiple phases:\n",
    "- `pending` - accepted by `k8s` cluster, but:\n",
    "    - one or more containers didn't start\n",
    "    - `POD` is waiting for node scheduling\n",
    "    - container image is currently downloaded\n",
    "- `running` - at least one `container` within `POD` is running (or being (re)started)\n",
    "- `succeeded` - all containers in the `POD` have suceeded __and will not restart__\n",
    "- `failed` - at least one container terminated in a failure\n",
    "- `unknown` - state of `POD` ould not be obtained, __typically communication error with `Node`__\n",
    "\n",
    "### Container states\n",
    "\n",
    "> __Kubernetes also watches the state of individual containers within `POD`s__\n",
    "\n",
    "Containers can be in one of three states:\n",
    "- `Waiting` - downloading image or pulling `secrets`, __reason is also provided for monitoring__\n",
    "- `Running`\n",
    "- `Terminated` - either terminated successfully or not; __reason and `exit code` is provided for monitoring__\n",
    "\n",
    "## Single Container POD\n",
    "\n",
    "> Most common use case is POD containing a single container\n",
    "\n",
    "In this case, we can think of a POD as a container wrapper\n",
    "\n",
    "Examples could include:\n",
    "- FastAPI server receiving requests and saving to shared database\n",
    "- Docker container receiving images as requests and forwarding the classification\n",
    "\n",
    "## Multiple Containers POD\n",
    "\n",
    "> More advanced use case, multiple tightly coupled containers __making a cohesive unit of service__\n",
    "\n",
    "![](./images/pod.svg)\n",
    "\n",
    "Examples could include:\n",
    "- Training multiple machine learning models, where:\n",
    "    - First container accesses shared storage of raw data and transforms it\n",
    "    - Second container trains neural network on the presented data\n",
    "    - Third container pushes out the trained model to serving container\n",
    "    - Fourth container serves the model\n",
    "- One container serving data to the public (`read only` persmissions), while another, internal one, writes data to shared storage\n",
    "\n",
    "> __POD containers are scheduled on the same \"logical host\" (for cloud, for clusters of servers: same VM or physical computer) due to tight coupling__\n",
    "\n",
    "## Defining POD (POD template)\n",
    "\n",
    "> `POD` as a basic `kind` can be specified via `.yaml` config file (__although specifying bare `POD`s is discouraged__)\n",
    "\n",
    "Specifying \"bare `POD`s\" is pretty straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25692fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: pod1\n",
    "  labels:\n",
    "    tier: frontend\n",
    "spec:\n",
    "  containers:\n",
    "  - name: hello1\n",
    "    image: gcr.io/google-samples/hello-app:2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b9103d",
   "metadata": {},
   "source": [
    "# Workload Resources\n",
    "\n",
    "> Kubernetes provides quite a few `workload resources` out of the box\n",
    "\n",
    "Before diving into specific `Workload Resources` let's get a few concepts straight:\n",
    "- __Each `workload` presented below uses `.spec.template` field WHICH SPECIFIES HOW TO CREATE A `POD`__\n",
    "- `template`is essneitally the same as `POD` config except `kind` and `apiVersion` (rest is performed the same)\n",
    "- __Each `workload` has a `.spec.selector` WHICH SPECIFIES WHICH `POD`s ARE HANDLED BY THE `WORKLOAD RESOURCE`__\n",
    "- `.spec.selector` use matches on defined `labels` __and may \"take care of\" `POD`s outside it's config file!__\n",
    "\n",
    "With the above in mind, let's dive in:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a932d5",
   "metadata": {},
   "source": [
    "## ReplicaSet\n",
    "\n",
    ">  Maintains a stable set of replica Pods running at any given time\n",
    "\n",
    "`ReplicaSet`:\n",
    "- creates new `POD`s accordingly to `.spec.replicas` field value\n",
    "    (from `.spec.template` config)\n",
    "- deletes `POD`s if too many of them are scheduled to nodes\n",
    "\n",
    "### Acquiring `POD`s\n",
    "\n",
    "> `ReplicaSet` is linked to the `POD`s via `metadata.ownerReferences` and acquired via `.spec.selector` match\n",
    "\n",
    "The above works something like this:\n",
    "- Each pod has `metadata.ownerReferences` __automatically added by `k8s`__\n",
    "- Above specifies __who manages the `POD`__ (e.g. another controller)\n",
    "- __If `POD`__:\n",
    "    - has no \"owner\" (e.g. bare `POD`) __or__\n",
    "    - it's owner __is not another controller and__\n",
    "    - `.spec.selector` fields match\n",
    "- Then `POD` is acquired by the `ReplicaSet`\n",
    "\n",
    "> __Above process works the same for other `workload resources` (or managers)__\n",
    "\n",
    "### When should we use `ReplicaSet`s?\n",
    "\n",
    "> __In general it is advised to use higher level `Deployment` `workload resource`__\n",
    "\n",
    "`Deployment`, a higher level concept, __manages `ReplicaSet`s__ and in addition provides __declarative updates to `POD`s__\n",
    "\n",
    "> Why is it still there?\n",
    "\n",
    "Only reasons to go for `ReplicaSet`s would be:\n",
    "- custom update orchestration\n",
    "- we will never need to update `config`\n",
    "\n",
    "Last one is pretty unlikely (and we are not paying large price for this feature), hence:\n",
    "\n",
    "> ### USE `DEPLOYMENT` INSTEAD\n",
    "\n",
    "(for those interested `ReplicaSet` is described in detail [here](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84952fe",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "> Provides declarative updates for `Pods` and `ReplicaSets`\n",
    "\n",
    "Given about information, let's see whether we can tell what each field means in the config below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d11ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: nginx-deployment\n",
    "  labels:\n",
    "    app: nginx\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: nginx\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: nginx\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: nginx\n",
    "        image: nginx:1.14.2\n",
    "        ports:\n",
    "        - containerPort: 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ccff8e",
   "metadata": {},
   "source": [
    "### Rollover\n",
    "\n",
    "What happens when `Deployment` is updated?\n",
    "- New `ReplicaSet` is created to bring up specified number of `POD`s\n",
    "- `POD`s whose `.spec.selector` matches __but `.spec.template` is different from new `Deployment`__ are scaled down\n",
    "- This process is performed until old deployment reaches `0` PODs and new one reaches `.spec.replicas` value\n",
    "\n",
    "If, in this process, we update `Deployment` once again a `rollover` will happen:\n",
    "\n",
    "> `rollover` means scaling down `ReplicaSet`s __as soon as new configuration comes in__\n",
    "\n",
    "For example:\n",
    "- First `Deployment` should create `5` PODs\n",
    "- `Deployment` is updated while only `3` PODs where scheduled\n",
    "- __`Deployment` will immediately scaled down `3` previous PODs AND WILL NOT WAIT FOR THE `5` TO COME UP!__\n",
    "\n",
    "### Label selector updates\n",
    "\n",
    "> __We SHOULD NOT update our `.spec.selector` and foresee how it should be designed__\n",
    "\n",
    "`.spec.selector` is even immutable for `api/v1` of `kubernetes`!\n",
    "\n",
    "To see possible drawbacks, check [this section](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#label-selector-updates).\n",
    "\n",
    "> Keep `selector`s of different controllers separate at all times!\n",
    "\n",
    "### Deployment status \n",
    "\n",
    "One of three states are generally available:\n",
    "- `progressing` - __rolling out new `ReplicaSet` to match config__\n",
    "- `complete` - desired state matches current\n",
    "- `fail to progress`, possible factors:\n",
    "    - Image pulling errors\n",
    "    - Insufficient permissions\n",
    "    - Insufficient resources on `Node`\n",
    "    \n",
    "For the last case we should specify `.spec.progressDeadlineSeconds=N` __which describes after how many seconds `Deployment` is considered stalled__\n",
    "\n",
    "It should be noted that:\n",
    "- __Above field DOES NOT END `Deployment` NOR `POD`!__\n",
    "- Higher level orchestrators can use this information though to act accordingly, for example:\n",
    "    - __`rollback` to previous `Deployment` settings__ (self-healing)\n",
    "- This info will be added to `k8s` managed `.status` field\n",
    "\n",
    "### Clean Up\n",
    "\n",
    "We can also control how many old `ReplicaSets` should be preserved via `.spec.revisionHistoryLimit`. \n",
    "\n",
    "> This setting is set to `10` by default in order to support up to `10` rollbacks in case something goes wrong!\n",
    "\n",
    "> __Setting this to `0` will erase possibility of `rollback`!__\n",
    "\n",
    "> __This is only kept as `ReplicaSet` generated config, NO NODE WILL RUN OUR OLD `Deployment`!__\n",
    "\n",
    "In general, leave at least value of `1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a28ced",
   "metadata": {},
   "source": [
    "## Writing `Deployment` spec\n",
    "\n",
    "Below are a few useful information about `.yml` config schema of `Deployment`:\n",
    "\n",
    "- `.spec.template.spec.restartPolicy=Always` is only allowed (e.g. `Deployment` will try to restart `POD`s indefinitely)\n",
    "- `.spec.replicas=1` is the default value\n",
    "- `.spec.selector` __MUST MATCH__ `.spec.template.metadata.` labels (__but can match more `POD`s hence you should look out__)\n",
    "- `.spec.strategy=\"RollingUpdate\"` is the default one (we saw description above):\n",
    "    - We can specify `\"Recreate\"` __which will kill all old `ReplicaSet`s before creating new ones__\n",
    "    - `.spec.strategy.rollingUpdate.maxUnavailable` - optional field; __percent, e.g. `=25%`__ of `POD`s which can be unavailable during update; __default: `25%`__\n",
    "    - `.spec.strategy.rollingUpdate.maxSurge` - __maximum number of `POD`s over the desired value__ (specification as above); __default: `25%`__\n",
    "\n",
    "\n",
    "Things to note:\n",
    "- Using `.spec.strategy.maxSurge` we can control how resource hungry is our application\n",
    "- Using `.spec.strategy.maxUnavailable` we can always ensure there are enough `POD`s to handle incoming traffics\n",
    "\n",
    "\n",
    "> ### `Deployment` should be used for stateless applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8f41d7",
   "metadata": {},
   "source": [
    "## DaemonSet\n",
    "\n",
    "> __`DaemonSet` ensures that `Node`s run a copy of `POD`s as they are added to the cluster__\n",
    "\n",
    "- When `Nodes` are removed `POD`s are also downscaled\n",
    "- Deleting `DaemonSet` makes `k8s` garbage collect the `POD`s\n",
    "\n",
    "### Usage\n",
    "\n",
    "- Running cluster storage `daemon` on every `node`\n",
    "- Running logs collection on every `node`\n",
    "- Running `Node` monitoring on every `node`\n",
    "\n",
    "> __Every long running `job` (daemon) per `Node` is a good fit for `DaemonSet`__\n",
    "\n",
    "### Required fields\n",
    "\n",
    "> Similar to `Deployment`, which means `.spec.template`, `.spec.selector` and __no `.spec.replicas` (as the same `daemon` is run per-node)__\n",
    "\n",
    "- Acquisition of `POD`s happens on matchin `.spec.selector` (like previously)\n",
    "- __Acquisition of `Node`s happens via `.spec.template.spec.nodeSelector` field__\n",
    "\n",
    "### Assigning Pods to Nodes\n",
    "\n",
    "> In general we don't have to interfere with `k8s` POD deployment to specific `Node`s\n",
    "\n",
    "There might be a few reasons to do that though:\n",
    "- Ensuring `POD` ends up on a `Node` which has `SSD` attached\n",
    "- Co-locate `POD`s from different services in the same zone if they communicate frequently\n",
    "\n",
    "`k8s` comes with a set of predefined `labels` for `Node`s, full list can be seen [here](https://kubernetes.io/docs/reference/labels-annotations-taints/), but to mention a few:\n",
    "- region of deployment (in case of cloud): `topology.kubernetes.io/region=us-east-1`\n",
    "- ip address of a node: `kubernetes.io/hostname=ip-172-20-114-199.ec2.internal`\n",
    "- operating system our node is running: `kubernetes.io/os=linux`\n",
    "\n",
    "> __During `kubectl` lesson we will learn how to add our own `label`s to `Node`s__\n",
    "\n",
    "Given the above, we can use the `.spec.template.spec.nodeSelector` as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b89771",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: DaemonSet\n",
    "metadata:\n",
    "  name: fluentd-elasticsearch\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      name: fluentd-elasticsearch\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        name: fluentd-elasticsearch\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: fluentd-elasticsearch\n",
    "        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2\n",
    "        resources:\n",
    "          limits:\n",
    "            memory: 200Mi\n",
    "          requests:\n",
    "            cpu: 100m\n",
    "            memory: 200Mi\n",
    "      terminationGracePeriodSeconds: 30\n",
    "      nodeSelector:\n",
    "        kubernetes.io/os!=linux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a4fd83",
   "metadata": {},
   "source": [
    "### Updating `DaemonSet`\n",
    "\n",
    "- __Intuitive per-node matching__: if we change `label`s `daemon`s will be added to newly matching __and removed from old `Node`s__\n",
    "- Deleting `DaemonSet` can be performed using `kubectl` normally (as we will see in the upcoming notebooks)\n",
    "- `POD`s are replaced according to `updateStrategy` mentioned in `Deployment`\n",
    "\n",
    "### Why `DaemonSet`s?\n",
    "\n",
    "One could also use `init` scripts on the nodes (even set up automatically during OS installation via [`packer`](https://www.packer.io/)), but:\n",
    "- It would be hard to monitor running daemons on multiple clusters\n",
    "- Different configs for `daemons` and `k8s` applications\n",
    "- Isolation between `daemons` and `apps` (although could be done otherwise)\n",
    "- Easier to create more complicated `daemon`s\n",
    "\n",
    "Versus deploying \"Bare `POD`s\":\n",
    "- `DaemonSet` replaces failed `daemons` accordingly to `restartStrategy`\n",
    "\n",
    "Versus `Deployment`:\n",
    "- Handles similiar loads (__jobs which are not supposed to terminate, e.g. `server`__)\n",
    "- `DaemonSet` allows us to choose specific `Node`s we would like the service to run on\n",
    "- `DaemonSet` does not need `replica`s as it works on a per-node basis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a00bc",
   "metadata": {},
   "source": [
    "## Jobs\n",
    "\n",
    "> Creates one or more `POD`s and will continue to retry execution of the `POD`s until a specified number of them successfully terminate\n",
    "\n",
    "Use cases:\n",
    "- create one `Job` to make sure our task runs successfully\n",
    "- run the same `Job` in parallel for `N` times\n",
    "\n",
    "An example config `Job` workload calculating `pi` value could be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae8dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: pi\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: pi\n",
    "        image: perl\n",
    "        command: [\"perl\",  \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n",
    "      restartPolicy: Never\n",
    "  backoffLimit: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1746b47b",
   "metadata": {},
   "source": [
    "### `.spec`ification\n",
    "\n",
    "Standard fields are necessary, but in addition:\n",
    "\n",
    "- `.spec.restartPolicy` can be either `Never` or `OnFailure` (default)\n",
    "- `.spec.completions` - how many `Job`s have to finish\n",
    "- `.spec.parallelism` - how many `POD`s with our job should be scheduled at the same time\n",
    "\n",
    "Using `.spec.completions` and `.spec.parallelism` we can construct different levels of parallelism:\n",
    "\n",
    "- __non-parallel__ - specify `.spec.completions=1`, only one `Job` will be created. __New one will only start after this one fails!__\n",
    "- __parallel with fixed completion count__ - specify `.spec.completions=N` to run __at most `N` parallel jobs at a given time__ (controller will reschedule `Node`s in case of failure)\n",
    "- __parallel with work queue__ - `.spec.completions=1` and `.spec.parallelism=N` - `N` PODs will run, after first one succeeds __the rest will continue execution until termination__ (for improved efficiency we need to implement direct `POD` to `POD` communication)\n",
    "\n",
    "`non-parallel` is the default mode as `.spec.completions=1` and `.spec.parallelism=1` are the default values.\n",
    "\n",
    "One could also specify [Completion Mode](https://kubernetes.io/docs/concepts/workloads/controllers/job/#completion-mode) which allows us to modify their behaviour upon termination.\n",
    "\n",
    "- `.spec.backoffLimit` - how many times __a single `POD`__ should be restarted before considering `Job` failed\n",
    "\n",
    "In this case:\n",
    "- Depending on the settings, but if `.spec.completions=N` is hit, __job succeeded__\n",
    "- Until this moment, try to recreate `POD` with `Job` `N` times\n",
    "- Exponential back-off delay is applied, e.g.:\n",
    "    - First retry after `10s`\n",
    "    - Second after `20s`\n",
    "    - Third after `40s`\n",
    "    - __Capped at `6m` backoff!__\n",
    "    \n",
    "- `.spec.activeDeadlineSeconds` - how many seconds __for the whole job__ until termination. Once reached, __all of the `POD`s ater terminated__ (takes precedence over `.spec.backoffLimit`)\n",
    "    \n",
    "### Cleanup\n",
    "\n",
    "> `Job` after finishing __will not be automatically removed from the cluster__\n",
    "\n",
    "Why is that bad?\n",
    "\n",
    "> `kube-apiserver` will still query `Job` and look for it's `POD`s __putting unneeded pressure on `k8s`__\n",
    "\n",
    "Why this is the default behaviour?\n",
    "- One might want to check logs of finished jobs (which are stored within `POD` or external storage)\n",
    "- Checking status of `Job`(s)\n",
    "\n",
    "What one can do is to set up a so called __`TTL` (time to live)__:\n",
    "\n",
    "> __`TTL` specifies after how long should the `job` be removed from the cluster (including all of it's `POD`s and other dependencies)__.\n",
    "\n",
    "One could do that via `.spec.ttlSecondsAfterFinished` field as one can read below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4527e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: pi-with-ttl\n",
    "spec:\n",
    "  ttlSecondsAfterFinished: 100\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: pi\n",
    "        image: perl\n",
    "        command: [\"perl\",  \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n",
    "      restartPolicy: Never"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fea0ec2",
   "metadata": {},
   "source": [
    "# Challenges\n",
    "\n",
    "> ### You should try these challenges after finishing all of the `kubernetes` related lessons!\n",
    "\n",
    "It will make your progress substantially faster.\n",
    "\n",
    "\n",
    "## Mandatory\n",
    "\n",
    "- What are [addons](https://kubernetes.io/docs/concepts/overview/components/#addons) in Kubernetes ecosystem? Which one is necessary?\n",
    "- Check out [kustomize](https://kubernetes.io/blog/2018/05/29/introducing-kustomize-template-free-configuration-customization-for-kubernetes/) (available via `kubectl`). What are the use cases for it? \n",
    "- See additional ways to match `POD`s to specific `Node`s [here](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)\n",
    "- Check out [`CronJob` workload resource](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/). Make sure you understand the concept and `cron` syntax included.\n",
    "\n",
    "## Additional\n",
    "\n",
    "- Check out [Kubernetes API Conventions](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md) to get deeper understanding of Kubernetes Objects and internals of the project.\n",
    "- What are the alternatives to `minikube`? What is `kind` or `k3s`? What are upsides/downsides of using them?\n",
    "- Check out [Client Libraries](https://kubernetes.io/docs/reference/using-api/client-libraries/) (which allow us to communicate with `kubelet-apiserver` via certain programming languages). Check out [Python Client Library](https://github.com/kubernetes-client/python/). Which task could you automate using it?\n",
    "- Check out [imperative commands way to manage `k8s` objects](https://kubernetes.io/docs/concepts/overview/working-with-objects/object-management/#imperative-commands), __but don't use it!__ What are the downsides due to which we did not describe it?\n",
    "- Read more about [TTL (time to live) Controller](https://kubernetes.io/docs/concepts/workloads/controllers/ttlafterfinished/).\n",
    "- Read about [canary `Deployment`s](https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments)\n",
    "- Read about [`Job` template expansion](https://kubernetes.io/docs/tasks/job/parallel-processing-expansion/)\n",
    "- What are `DaemonSet` tolerations?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash [conda env:.conda-AiCore] *",
   "language": "bash",
   "name": "conda-env-.conda-AiCore-bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
