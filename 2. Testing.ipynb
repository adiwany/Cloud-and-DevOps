{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "amazing-preference",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "One of the essentials when the time comes to deploy our code (or models) is ensuring it works how we intended.\n",
    "\n",
    "> __Testing allows us to have higher confidence in how our code works__\n",
    "\n",
    "There are a lot of approaches we will briefly go over some of them before diving into __testing Python code specifically__.\n",
    "\n",
    "## Functional testing\n",
    "\n",
    "> __Functional testing describes testing the functionality of our product (code, mobile app, UI etc.) manually__\n",
    "\n",
    "There are a few steps one can perform here (and are conducted in this order), each being a \"higher level approach\":\n",
    "- __`unit testing` - tells us WHERE and HOW something failed__:\n",
    "    - usually done by the developers themselves\n",
    "    - testing specific functions/methods to see that they return assumed values/do assumed things\n",
    "- __`integration testing` - tells us WHAT failed__:\n",
    "    - framed as scenarios (e.g. user wants to log something into file)\n",
    "    - tests how different units cooperate with each other\n",
    "- __`system testing`__:\n",
    "    - whole system/product is tested against desired functionality\n",
    "    - considered as a black box\n",
    "    - usually done by specialized testing team\n",
    "- __`acceptance testing`__:\n",
    "    - rolling out to production \n",
    "    - beta testing (your end users)\n",
    "    - verifying requirements of the product are really met    \n",
    "    \n",
    "> __Some of those tests could be generated which saves hours of work if done correctly!__\n",
    "\n",
    "## Non-functional testing\n",
    "\n",
    "> __Testing various components of the systems not directly related with desired functionality__\n",
    "\n",
    "### Performance testing\n",
    "\n",
    "> __How performant our product is under different conditions__\n",
    "\n",
    "During those tests there are a few key things to keep in mind:\n",
    "- __Find bottlenecks__ - where your code takes absurdly long to run (maybe it is a single slow operation you can change?)\n",
    "- __Premature optimization is the root of all evil__ - if it is fast enough, don't try to improve by `0.1%`\n",
    "- __Test under different load__, some examples:\n",
    "    - How the performance differs based on increased batch size?\n",
    "    - __Spike testing__: What if our machine learning app deployed on AWS has a sudden user spike?\n",
    "    - __Stress testing__: how your product behaves at __or even above__ it's limits (e.g. large input values, large data, large traffic), is this how you envisioned it?\n",
    "    - __Endurance testing__: normal load but for a long time; how often is your web app down?\n",
    "    \n",
    "    \n",
    "### Security testing\n",
    "\n",
    "> __Keep in mind this topic is way too broad and worthy of another course on it's own!__\n",
    "\n",
    "Importance of this topic is often underestimated, but it is an essential piece of many infrastructures.\n",
    "Few things you should keep in mind:\n",
    "- __Minimum trust approach__ - give only absolutely necessary permissions to users/coworkers\n",
    "- __Separate roles__ - permissions only related to their roles\n",
    "- __Try to break it__ - check out pentesting or ethical hacking\n",
    "\n",
    "### Compatibility testing\n",
    "\n",
    "> __How compatible is our product with previous iteration and/or different environments__\n",
    "\n",
    "Luckily the second type of compatibility can be simply improved by using `docker` (__principle of shifting responsibility to providers__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-variety",
   "metadata": {},
   "source": [
    "## Other helpful techniques\n",
    "\n",
    "> In order to keep your code in check one can employ a few simple additional techniques\n",
    "\n",
    "- __Peer review__ - each thing you do is checked by another person:\n",
    "    - Pull Requests are often checked by assigned reviewers\n",
    "    - Scientific papers are under double blind peer-review\n",
    "- __Code analyzers__ - GitHub offers a lot of integrations, __which looks for possible bugs in your code automatically__, a few examples with easy integration:\n",
    "    - [`codebeat`](https://codebeat.co/)\n",
    "    - [`sonarqube`](https://www.sonarqube.org/)\n",
    "    - [`codacy`](https://www.codacy.com/)\n",
    "    - [`codeclimate`](https://codeclimate.com/) \n",
    "- __Test coverage__ - how many (in percentage) of our code was tested. One can obtain it via [`coverage.py`](https://coverage.readthedocs.io/en/coverage-5.5/) with testing framework of choice (also it is possible to integrate with GitHub Actions, which we will see in a few lessons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-archive",
   "metadata": {},
   "source": [
    "## unittest\n",
    "\n",
    "> `unittest` is a built-in `python` module which allows us to write unit tests efficiently\n",
    "\n",
    "Module structure is very simple and best explained by an example we can go over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fitting-manitoba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T14:19:54.627788Z",
     "start_time": "2021-04-19T14:19:54.611464Z"
    }
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestStringMethods(unittest.TestCase):\n",
    "\n",
    "    def test_upper(self):\n",
    "        self.assertEqual('foo'.upper(), 'FOO')\n",
    "\n",
    "    def test_isupper(self):\n",
    "        self.assertTrue('FOO'.isupper())\n",
    "        self.assertFalse('Foo'.isupper())\n",
    "\n",
    "    def test_split(self):\n",
    "        s = 'hello world'\n",
    "        self.assertEqual(s.split(), ['hello', 'world'])\n",
    "        # check that s.split fails when the separator is not a string\n",
    "        with self.assertRaises(TypeError):\n",
    "            s.split(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adverse-relations",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T14:19:54.642720Z",
     "start_time": "2021-04-19T14:19:54.629507Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "test_obvious (__main__.DummyFailedTest) ... ok\n",
      "test_stupid (__main__.DummyFailedTest) ... FAIL\n",
      "test_length (__main__.FileTestWithHandle) ... ok\n",
      "test_maybe_skipped (__main__.MyTestCase) ... skipped 'Yes, 5 is equal to 5 so we skip'\n",
      "test_nothing (__main__.MyTestCase) ... skipped 'demonstrating skipping'\n",
      "test_py3_format (__main__.MyTestCase) ... ok\n",
      "test_windows_support (__main__.MyTestCase) ... skipped 'Windows required'\n",
      "test_decode_inverts_encode (__main__.TestEncoding) ... ok\n",
      "test_isupper (__main__.TestStringMethods) ... ok\n",
      "test_split (__main__.TestStringMethods) ... ok\n",
      "test_upper (__main__.TestStringMethods) ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_stupid (__main__.DummyFailedTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-19-303e6c82e795>\", line 6, in test_stupid\n",
      "    self.assertEqual(\"foo\", \"bar\")\n",
      "AssertionError: 'foo' != 'bar'\n",
      "- foo\n",
      "+ bar\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 11 tests in 0.314s\n",
      "\n",
      "FAILED (failures=1, skipped=3)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fda4123ed60>"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "# In your Python code you should simply run unittest.main()\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-configuration",
   "metadata": {},
   "source": [
    "As one could see above, `unittest` consists of:\n",
    "- `classes` inheriting from `unittest.TestCase` - those should contain semantically related tests\n",
    "- method(s) of said class which run a specific unit test\n",
    "- `unittest.main` function which parses Python file looking for test cases (as specified by `classes` and `methods`)\n",
    "\n",
    "There are also a few constructs which act like `assert` statement, __each of those has to pass__.\n",
    "\n",
    "Let's see how a failed test case looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "conscious-heaven",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T14:19:54.657342Z",
     "start_time": "2021-04-19T14:19:54.645062Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "test_obvious (__main__.DummyFailedTest) ... ok\n",
      "test_stupid (__main__.DummyFailedTest) ... FAIL\n",
      "test_length (__main__.FileTestWithHandle) ... ok\n",
      "test_maybe_skipped (__main__.MyTestCase) ... skipped 'Yes, 5 is equal to 5 so we skip'\n",
      "test_nothing (__main__.MyTestCase) ... skipped 'demonstrating skipping'\n",
      "test_py3_format (__main__.MyTestCase) ... ok\n",
      "test_windows_support (__main__.MyTestCase) ... skipped 'Windows required'\n",
      "test_decode_inverts_encode (__main__.TestEncoding) ... ok\n",
      "test_isupper (__main__.TestStringMethods) ... ok\n",
      "test_split (__main__.TestStringMethods) ... ok\n",
      "test_upper (__main__.TestStringMethods) ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_stupid (__main__.DummyFailedTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-38-303e6c82e795>\", line 6, in test_stupid\n",
      "    self.assertEqual(\"foo\", \"bar\")\n",
      "AssertionError: 'foo' != 'bar'\n",
      "- foo\n",
      "+ bar\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 11 tests in 0.326s\n",
      "\n",
      "FAILED (failures=1, skipped=3)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fda412267f0>"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "class DummyFailedTest(unittest.TestCase):\n",
    "    def test_obvious(self):\n",
    "        self.assertEqual(\"foo\", \"foo\")\n",
    "\n",
    "    def test_stupid(self):\n",
    "        self.assertEqual(\"foo\", \"bar\")\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-facility",
   "metadata": {},
   "source": [
    "Please notice __all of the tests in this notebook were run__.\n",
    "\n",
    "### setUp() and tearDown()\n",
    "\n",
    "One can add two more methods:\n",
    "- `setUp()` - runs before test cases in the classes, sets up necessary stuff like reading files, connecting to database etc.\n",
    "- `tearDown()` - runs after test cases to destroy leftovers created by tests\n",
    "\n",
    "Simple example could be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "twelve-failing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T14:19:54.672182Z",
     "start_time": "2021-04-19T14:19:54.659019Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "test_obvious (__main__.DummyFailedTest) ... ok\n",
      "test_stupid (__main__.DummyFailedTest) ... FAIL\n",
      "test_length (__main__.FileTestWithHandle) ... ok\n",
      "test_maybe_skipped (__main__.MyTestCase) ... skipped 'Yes, 5 is equal to 5 so we skip'\n",
      "test_nothing (__main__.MyTestCase) ... skipped 'demonstrating skipping'\n",
      "test_py3_format (__main__.MyTestCase) ... ok\n",
      "test_windows_support (__main__.MyTestCase) ... skipped 'Windows required'\n",
      "test_decode_inverts_encode (__main__.TestEncoding) ... ok\n",
      "test_isupper (__main__.TestStringMethods) ... ok\n",
      "test_split (__main__.TestStringMethods) ... ok\n",
      "test_upper (__main__.TestStringMethods) ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_stupid (__main__.DummyFailedTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-38-303e6c82e795>\", line 6, in test_stupid\n",
      "    self.assertEqual(\"foo\", \"bar\")\n",
      "AssertionError: 'foo' != 'bar'\n",
      "- foo\n",
      "+ bar\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 11 tests in 0.205s\n",
      "\n",
      "FAILED (failures=1, skipped=3)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fda40d36d60>"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class FileTestWithHandle(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.handle = open(\"./0. Docker/__main__.py\", \"r\")\n",
    "\n",
    "    def test_length(self):\n",
    "        length = len(self.handle.readlines())\n",
    "        self.assertTrue(length > 20)\n",
    "\n",
    "    def tearDown(self):\n",
    "        self.handle.close()\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-mambo",
   "metadata": {},
   "source": [
    "### Skipping tests\n",
    "\n",
    "Using decorators we can further control tests we are running (e.g. skip some of them based on condition).\n",
    "\n",
    "See options below (`unittest.skip` can also be used as a decorator on the whole class a.k.a. __test suite__):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "median-aquatic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T14:19:54.688793Z",
     "start_time": "2021-04-19T14:19:54.673570Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "test_obvious (__main__.DummyFailedTest) ... ok\n",
      "test_stupid (__main__.DummyFailedTest) ... FAIL\n",
      "test_length (__main__.FileTestWithHandle) ... ok\n",
      "test_maybe_skipped (__main__.MyTestCase) ... skipped 'Yes, 5 is equal to 5 so we skip'\n",
      "test_nothing (__main__.MyTestCase) ... skipped 'demonstrating skipping'\n",
      "test_py3_format (__main__.MyTestCase) ... ok\n",
      "test_windows_support (__main__.MyTestCase) ... skipped 'Windows required'\n",
      "test_decode_inverts_encode (__main__.TestEncoding) ... ok\n",
      "test_isupper (__main__.TestStringMethods) ... ok\n",
      "test_split (__main__.TestStringMethods) ... ok\n",
      "test_upper (__main__.TestStringMethods) ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_stupid (__main__.DummyFailedTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-38-303e6c82e795>\", line 6, in test_stupid\n",
      "    self.assertEqual(\"foo\", \"bar\")\n",
      "AssertionError: 'foo' != 'bar'\n",
      "- foo\n",
      "+ bar\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 11 tests in 0.184s\n",
      "\n",
      "FAILED (failures=1, skipped=3)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fda414954c0>"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "class MyTestCase(unittest.TestCase):\n",
    "    @unittest.skip(\"demonstrating skipping\")\n",
    "    def test_nothing(self):\n",
    "        # This line will not run at all\n",
    "        self.fail(\"shouldn't happen\")\n",
    "\n",
    "    @unittest.skipIf(sys.version_info.major < 3, \"Not supported for Python 2\")\n",
    "    def test_py3_format(self):\n",
    "        self.assertEqual(\"{}\".format(\"aaa\"), \"aaa\")\n",
    "        pass\n",
    "\n",
    "    @unittest.skipUnless(sys.platform.startswith(\"win\"), \"Windows required\")\n",
    "    def test_windows_support(self):\n",
    "        # windows specific testing code\n",
    "        pass\n",
    "\n",
    "    def test_maybe_skipped(self):\n",
    "        # Skip test from within the function body\n",
    "        if 5 == 5:\n",
    "            self.skipTest(\"Yes, 5 is equal to 5 so we skip\")\n",
    "        # test code which would run if 5 != 5 (essentially never, we know)\n",
    "        ...\n",
    "        \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-recognition",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "- Read the following code and create unit tests to find out implementation bugs\n",
    "- Fix the code below accordingly to found bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dramatic-transparency",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T14:19:54.881455Z",
     "start_time": "2021-04-19T14:19:54.690087Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(data: np.ndarray):\n",
    "    return 1 / (1 + np.exp(data))\n",
    "\n",
    "def softmax(x):\n",
    "    exponentials = np.exp(x)\n",
    "    return exponentials / exponentials.sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-adelaide",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "> __Hypothesis is a `python` library for `property-based testing` which is easier, more powerful and has way larger test cases coverage than standard unit testing__\n",
    "\n",
    "Difference between `unit testing` and `property-based testing` is fantastically explained by [Hypothesis Welcome Page](https://hypothesis.readthedocs.io/en/latest/):\n",
    "\n",
    "__Think of a normal unit test as being something like the following:__\n",
    "\n",
    "1. Set up some data.\n",
    "2. Perform some operations on the data.\n",
    "3. Assert something about the result.\n",
    "\n",
    "__Hypothesis lets you write tests which instead look like this:__\n",
    "\n",
    "1. For all data matching some specification.\n",
    "2. Perform some operations on the data.\n",
    "3. Assert something about the result.\n",
    "\n",
    "This idea was popularized by [Haskell](https://www.haskell.org/) (purely functional programming language) library [QuickCheck](https://hackage.haskell.org/package/QuickCheck).\n",
    "\n",
    "> __Hypothesis generates testing data based on your specification and checks whether guarantees you want to give hold true.__\n",
    "\n",
    "### Installation\n",
    "\n",
    "As per usual, one can install `hypothesis` via `pip` or [`conda`](https://anaconda.org/conda-forge/hypothesis).\n",
    "\n",
    "There are also a few extensions provided for scientific stack especially (like `numpy` or `pandas`).\n",
    "\n",
    "To install `hypothesis` with `numpy` generation strategies via `pip` one could do (__and of course you should also inside your `AiCore` `conda` environment__):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "attractive-trinidad",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "zsh:1: no matches found: hypothesis[numpy]\n"
     ]
    }
   ],
   "source": [
    "!pip install hypothesis[numpy]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-samuel",
   "metadata": {},
   "source": [
    "### General\n",
    "\n",
    "First, let's set up an example which `encode`s the string and `decode`s it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "efficient-portland",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T14:19:54.888093Z",
     "start_time": "2021-04-19T14:19:54.883595Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode(input_string):\n",
    "    count = 1\n",
    "    prev = \"\"\n",
    "    lst = []\n",
    "    for character in input_string:\n",
    "        if character != prev:\n",
    "            if prev:\n",
    "                entry = (prev, count)\n",
    "                lst.append(entry)\n",
    "            count = 1\n",
    "            prev = character\n",
    "        else:\n",
    "            count += 1\n",
    "    entry = (character, count)\n",
    "    lst.append(entry)\n",
    "    return lst\n",
    "\n",
    "\n",
    "def decode(lst):\n",
    "    q = \"\"\n",
    "    for character, count in lst:\n",
    "        q += character * count\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-alcohol",
   "metadata": {},
   "source": [
    "It should be fairly obvious, that `encode(decode(<string>))` should return original `<string>`.\n",
    "\n",
    "Hypothesis can generate `<string>` examples for us (just like unit tests, but way easier and automated) using:\n",
    "- `strategy` - way to create testing data (in this case `text`)\n",
    "- `given` - generate samples from the specified strategy\n",
    "\n",
    "With that in mind, let's see how we could do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "comparative-trinidad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T14:19:54.963781Z",
     "start_time": "2021-04-19T14:19:54.891293Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "test_obvious (__main__.DummyFailedTest) ... ok\n",
      "test_stupid (__main__.DummyFailedTest) ... FAIL\n",
      "test_length (__main__.FileTestWithHandle) ... ok\n",
      "test_maybe_skipped (__main__.MyTestCase) ... skipped 'Yes, 5 is equal to 5 so we skip'\n",
      "test_nothing (__main__.MyTestCase) ... skipped 'demonstrating skipping'\n",
      "test_py3_format (__main__.MyTestCase) ... ok\n",
      "test_windows_support (__main__.MyTestCase) ... skipped 'Windows required'\n",
      "test_decode_inverts_encode (__main__.TestEncoding) ... ERROR\n",
      "test_isupper (__main__.TestStringMethods) ... ok\n",
      "test_split (__main__.TestStringMethods) ... ok\n",
      "test_upper (__main__.TestStringMethods) ... Falsifying example: test_decode_inverts_encode(\n",
      "    s='', self=<__main__.TestEncoding testMethod=test_decode_inverts_encode>,\n",
      ")\n",
      "ok\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_decode_inverts_encode (__main__.TestEncoding)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-44-d413d060cb54>\", line 8, in test_decode_inverts_encode\n",
      "    def test_decode_inverts_encode(self, s):\n",
      "  File \"/Users/ice/miniconda3/envs/main/lib/python3.8/site-packages/hypothesis/core.py\", line 1169, in wrapped_test\n",
      "    raise the_error_hypothesis_found\n",
      "  File \"<ipython-input-44-d413d060cb54>\", line 9, in test_decode_inverts_encode\n",
      "    self.assertEqual(decode(encode(s)), s)\n",
      "  File \"<ipython-input-43-79a3830c57ce>\", line 14, in encode\n",
      "    entry = (character, count)\n",
      "UnboundLocalError: local variable 'character' referenced before assignment\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_stupid (__main__.DummyFailedTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-38-303e6c82e795>\", line 6, in test_stupid\n",
      "    self.assertEqual(\"foo\", \"bar\")\n",
      "AssertionError: 'foo' != 'bar'\n",
      "- foo\n",
      "+ bar\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 11 tests in 0.071s\n",
      "\n",
      "FAILED (failures=1, errors=1, skipped=3)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fda415279a0>"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "# Change that to unittest\n",
    "\n",
    "from hypothesis import given\n",
    "import hypothesis.strategies as st\n",
    "\n",
    "class TestEncoding(unittest.TestCase):\n",
    "    @given(st.text())\n",
    "    def test_decode_inverts_encode(self, s):\n",
    "        self.assertEqual(decode(encode(s)), s)\n",
    "        \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-twelve",
   "metadata": {},
   "source": [
    "First of all notice how easy it is to mix `unittest` with `hypothesis` to create way more comprehensive test suite.\n",
    "\n",
    "You can see that for our string encoding function fails when the input is an empty string. If we fix the above code by appending\n",
    "the check for empty string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "adolescent-dublin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T14:19:54.965726Z",
     "start_time": "2021-04-19T14:19:54.645Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode(input_string):\n",
    "    # This is an example fix\n",
    "    if input_string == \"\":\n",
    "        return []\n",
    "    \n",
    "    count = 1\n",
    "    prev = \"\"\n",
    "    lst = []\n",
    "    for character in input_string:\n",
    "        if character != prev:\n",
    "            if prev:\n",
    "                entry = (prev, count)\n",
    "                lst.append(entry)\n",
    "            count = 1\n",
    "            prev = character\n",
    "        else:\n",
    "            count += 1\n",
    "    entry = (character, count)\n",
    "    lst.append(entry)\n",
    "    return lst\n",
    "\n",
    "\n",
    "def decode(lst):\n",
    "    q = \"\"\n",
    "    for character, count in lst:\n",
    "        q += character * count\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-subscriber",
   "metadata": {},
   "source": [
    "And re-running the test (`@example` specifies this example will always be run, good for catching edge cases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "powerful-europe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T14:19:54.966814Z",
     "start_time": "2021-04-19T14:19:54.657Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "test_obvious (__main__.DummyFailedTest) ... ok\n",
      "test_stupid (__main__.DummyFailedTest) ... FAIL\n",
      "test_length (__main__.FileTestWithHandle) ... ok\n",
      "test_maybe_skipped (__main__.MyTestCase) ... skipped 'Yes, 5 is equal to 5 so we skip'\n",
      "test_nothing (__main__.MyTestCase) ... skipped 'demonstrating skipping'\n",
      "test_py3_format (__main__.MyTestCase) ... ok\n",
      "test_windows_support (__main__.MyTestCase) ... skipped 'Windows required'\n",
      "test_decode_inverts_encode (__main__.TestEncoding) ... ok\n",
      "test_isupper (__main__.TestStringMethods) ... ok\n",
      "test_split (__main__.TestStringMethods) ... ok\n",
      "test_upper (__main__.TestStringMethods) ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_stupid (__main__.DummyFailedTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-38-303e6c82e795>\", line 6, in test_stupid\n",
      "    self.assertEqual(\"foo\", \"bar\")\n",
      "AssertionError: 'foo' != 'bar'\n",
      "- foo\n",
      "+ bar\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 11 tests in 0.220s\n",
      "\n",
      "FAILED (failures=1, skipped=3)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7fda414aabb0>"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-shopping",
   "metadata": {},
   "source": [
    "Generated tests pass correctly.\n",
    "\n",
    "> __Hypothesis is smart about running tests, IT WILL ONLY RUN THE FAILED CASES (as it has it's own internal database)!__\n",
    "\n",
    "### Hypothesis tricks\n",
    "\n",
    "A few useful things, which should help you with your tests:\n",
    "\n",
    "> __`filter` values generated by a strategy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "relevant-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "@given(st.integers().filter(lambda x: x % 2 == 0))\n",
    "def test_even_integers(i):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-shareware",
   "metadata": {},
   "source": [
    "> __`assume` that input is/is not something__\n",
    "\n",
    "> __NOTE:__ Hypothesis will fail if your assumptions get rid of too many generated samples\n",
    "\n",
    "Test will not be marked as `failing` __if the assumption is `false`__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "hindu-college",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import isnan\n",
    "\n",
    "@given(st.floats())\n",
    "def test_negation_is_self_inverse_for_non_nan(x):\n",
    "    assume(not isnan(x))\n",
    "    assert x == -(-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-catering",
   "metadata": {},
   "source": [
    "> __strategies are highly customizable__\n",
    "\n",
    "One can specify a lot of parameters for the strategies, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "hired-population",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "st.integers(min_value=0, max_value=10).example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-orchestra",
   "metadata": {},
   "source": [
    "> __`given` can specify some/all argument to function via `kwargs` or `args`__\n",
    "\n",
    "With the following signature:\n",
    "\n",
    "```\n",
    "hypothesis.given(*_given_arguments, **_given_kwargs)\n",
    "```\n",
    "\n",
    "Valid cases could be (amongst others):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "conceptual-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "@given(st.integers(), st.integers())\n",
    "def a(x, y):\n",
    "    pass\n",
    "\n",
    "\n",
    "@given(st.integers())\n",
    "def b(x, y):\n",
    "    pass\n",
    "\n",
    "\n",
    "@given(y=st.integers())\n",
    "def c(x, y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-crest",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "- Once again, test `sigmoid` and `softmax` functions, this time using `hypothesis` strategies\n",
    "- Check out [documentation](https://hypothesis.readthedocs.io/en/latest/numpy.html#hypothesis.extra.numpy.arrays) to see how to generate `np.ndarray` instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-quebec",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "### Assessment\n",
    "\n",
    "- Learn basics of `pytest` (documentation [here](https://docs.pytest.org/en/latest/contents.html))\n",
    "- What is `pytest`'s [mark.parametrize](https://docs.pytest.org/en/stable/parametrize.html)? \n",
    "- What is [Test Driven Development](https://en.wikipedia.org/wiki/Test-driven_development) and what are the general steps needed to follow this approach?\n",
    "\n",
    "### Non-assessment\n",
    "\n",
    "- Check out [`doctest`](https://docs.python.org/3/library/doctest.html#module-doctest) Python module, which allows you to test your code placed inside `docstring`s (you can think of it like a \"smoke\" tests, which only test whether everything runs correctly) \n",
    "- Read more about automated test case generation via [Hypothesis](https://hypothesis.readthedocs.io/en/latest/). We don't want to do more manual work than needed, do we?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python386jvsc74a57bd006c1e258a470a687113bfba03f207c092b27379067ada2d83b8b31269ab641fe",
   "display_name": "Python 3.8.6 64-bit ('main': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}